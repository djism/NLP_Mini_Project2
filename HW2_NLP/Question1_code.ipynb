{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.1 A Baseline Neural Network Tagger (40 points)\n",
        "\n",
        "Train a feed-forward neural network classifier to predict the POS tag of a word in its context. The input\n",
        "should be the word embedding for the center word concatenated with the word embeddings for words\n",
        "in a context window. We’ll define a context window as the sequence of words containing w words\n",
        "to either side of the center word and including the center word itself, so the context window contains\n",
        "1 + 2w words in total. For example, if w = 1 and the word embedding dimensionality is d, the total\n",
        "dimensionality of the input will be 3d. For words near the sentence boundaries, pad the sentence with\n",
        "beginning-of-sentence and end-of-sentence characters. The word embeddings should be randomly initialized and learned along with all other parameters in the model.\n",
        "\n",
        "**functional architecture**: The input is the concatenation of word embeddings in the context window,\n",
        "with the word to be tagged in the center. Use a single hidden layer of width 128 with a tanh nonlinearity.\n",
        "The hidden layer should then be fed to an affine transformation which will produce scores for all possible POS tags. Use a softmax transformation on the scores to produce a probability distribution over tags.\n",
        "\n",
        "**learning**: Use log loss as the objective function (log loss is often called “cross entropy” or “negative\n",
        "log-likelihood” when training neural networks, so those terms may be useful when searching for the\n",
        "right loss function in toolkits). Use SGD or any other optimizer you wish. Toolkits typically have many optimizers already implemented.\n",
        "\n",
        "**initialization**: Randomly initialize all parameters, including word embeddings, and train them. Note\n",
        "that embeddings for words that only appear in DEV/DEVTEST will not be trained at all. So, you need to\n",
        "be careful about how those embeddings are set in order to get good results. We suggest an initialization\n",
        "range of -0.01 to 0.01 for all word embedding parameters. (You could alternatively try setting embeddings for unknown words to all zeros or try learning an unknown word embedding during training.)\n",
        "Train on TRAIN, perform early stopping and preliminary testing on DEV, and report your final\n",
        "tagging accuracy on DEVTEST. Report results with both w = 0 and w = 1. Submit your code.\n",
        "\n",
        "**Notes**: With w = 0, I was seeing a best DEV accuracy of 77-78% and with w = 1 it improved to 80-81%. I\n",
        "set the size (dimensionality) of word embeddings to 50, and used SGD with a fixed step size of 0.02 and\n",
        "each mini-batch contained one word to be tagged. I trained for 10 epochs and evaluated on DEV once\n",
        "per epoch. It took approximately 10 seconds per epoch using PyTorch on a 3.3 GHz Intel Core i5."
      ],
      "metadata": {
        "id": "rtnoYwNNnMVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Here Initializing the embeddings randomly within range (-0.01, 0.01)\n",
        "def initialize_embeddings(vocab_size, embedding_dim):\n",
        "    embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "    embeddings.weight.data.uniform_(-0.01, 0.01)\n",
        "    return embeddings\n",
        "\n",
        "# Here Loading the data from the files\n",
        "def load_data(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        tweet = []\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                word, pos = line.strip().split('\\t')\n",
        "                tweet.append((word, pos))\n",
        "            else:\n",
        "                if tweet:\n",
        "                    data.append(tweet)\n",
        "                tweet = []\n",
        "        if tweet:\n",
        "            data.append(tweet)\n",
        "    return data\n",
        "\n",
        "# Here indexing the different POS tags\n",
        "def pos_to_index():\n",
        "    pos_tags = [\"N\", \"O\", \"S\", \"L\", \"^\", \"Z\", \"M\", \"V\", \"A\", \"R\", \"!\", \"D\", \"P\", \"&\", \"T\", \"X\", \"Y\", \"#\", \"@\", \"~\", \"U\", \"E\", \"$\", \",\", \"G\"]\n",
        "    return {tag: idx for idx, tag in enumerate(pos_tags)}\n",
        "\n",
        "# Here Converting words to indices\n",
        "def get_word_index(word, word_to_idx):\n",
        "    return word_to_idx.get(word, word_to_idx['UUUNKKK'])\n",
        "\n",
        "# Code for getting context window for a base word in a tweet\n",
        "def get_context(tweet, index, window_size=1):\n",
        "    padded_tweet = ['<s>'] * window_size + [word for word, pos in tweet] + ['</s>'] * window_size\n",
        "    context_start = index\n",
        "    context_end = index + 2 * window_size + 1\n",
        "    return padded_tweet[context_start:context_end]\n",
        "\n",
        "# Code for creating input vectors by concatenating word indices in the context window\n",
        "def get_input_vector(context, word_to_idx):\n",
        "    return torch.tensor([get_word_index(word, word_to_idx) for word in context], dtype=torch.long)"
      ],
      "metadata": {
        "id": "L8B4vUkUD4qp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Define the model class for Feed Forward N.N.\n",
        "class POSModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, window_size):\n",
        "        super(POSModel, self).__init__()\n",
        "        self.embedding = initialize_embeddings(vocab_size, embedding_dim)\n",
        "        input_dim = embedding_dim * (2 * window_size + 1)\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeds = self.embedding(x)\n",
        "        x = embeds.view(1, -1)\n",
        "        x = self.tanh(self.fc1(x))\n",
        "        scores = self.fc2(x)\n",
        "        return scores"
      ],
      "metadata": {
        "id": "BWpGqXU6EG5c"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Code for training the Model\n",
        "def train_model(model, train_data, dev_data, word_to_idx, pos_dict, window_size, epochs=10, lr=0.02):\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "\n",
        "        for tweet in train_data:\n",
        "            for i, (word, pos) in enumerate(tweet):\n",
        "                context = get_context(tweet, i, window_size)\n",
        "                input_vector = get_input_vector(context, word_to_idx)\n",
        "                target = torch.tensor([pos_dict[pos]], dtype=torch.long)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                output = model(input_vector)\n",
        "\n",
        "                loss = criterion(output, target)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}')\n",
        "        evaluate_model(model, dev_data, word_to_idx, pos_dict, window_size, 4821)\n",
        "\n",
        "# Code for evaluating the model\n",
        "def evaluate_model(model, test_data, word_to_idx, pos_dict, window_size, expected_count):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for tweet in test_data:\n",
        "            for i, (word, pos) in enumerate(tweet):\n",
        "                context = get_context(tweet, i, window_size)\n",
        "                input_vector = get_input_vector(context, word_to_idx)\n",
        "                target = pos_dict[pos]\n",
        "\n",
        "                output = model(input_vector)\n",
        "                predicted = torch.argmax(output).item()\n",
        "                if predicted == target:\n",
        "                    correct += 1\n",
        "                total += 1\n",
        "\n",
        "    assert total == expected_count, f\"Expected {expected_count} predictions, but got {total}\"\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "uU-yo7SaEPeK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# A utility function to read files and setting dimensions and then calling the model to run on this data\n",
        "def result(w):\n",
        "    vocab = set()\n",
        "    train_data = load_data('/twpos-train.tsv')\n",
        "    dev_data = load_data('/twpos-dev.tsv')\n",
        "    devtest_data = load_data('/twpos-devtest.tsv')\n",
        "\n",
        "    for tweet in train_data:\n",
        "        for word, _ in tweet:\n",
        "            vocab.add(word)\n",
        "\n",
        "    word_to_idx = {word: idx for idx, word in enumerate(vocab, start=1)}\n",
        "    word_to_idx['<s>'] = 0\n",
        "    word_to_idx['</s>'] = 0\n",
        "    word_to_idx['UUUNKKK'] = len(vocab) + 1\n",
        "\n",
        "\n",
        "    pos_dict = pos_to_index()\n",
        "\n",
        "\n",
        "    vocab_size = len(word_to_idx)\n",
        "    embedding_dim = 50\n",
        "    hidden_dim = 128\n",
        "    output_dim = len(pos_dict)\n",
        "    window_size = w\n",
        "\n",
        "\n",
        "    model = POSModel(vocab_size, embedding_dim, hidden_dim, output_dim, window_size)\n",
        "\n",
        "\n",
        "    model.embedding.weight.data[word_to_idx['UUUNKKK']] = 0\n",
        "    model.embedding.weight.data[word_to_idx['<s>']] = 0\n",
        "    model.embedding.weight.data[word_to_idx['</s>']] = 0\n",
        "\n",
        "    # Code to call Train and evaluate function for the model\n",
        "    train_model(model, train_data, dev_data, word_to_idx, pos_dict, window_size, epochs=10)\n",
        "    print(\"Evaluating on DEVTEST set...\")\n",
        "    evaluate_model(model, devtest_data, word_to_idx, pos_dict, window_size, 4639)"
      ],
      "metadata": {
        "id": "kwJr1A6aEckC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def main():\n",
        "    context_window = [0, 1]\n",
        "    for w in context_window:\n",
        "        result(w)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrpAsNsD_dMj",
        "outputId": "95beff31-41e9-46dc-af6c-a131d2e2839e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 26006.6902\n",
            "Accuracy: 0.7094\n",
            "Epoch 2/10, Loss: 12139.1604\n",
            "Accuracy: 0.7693\n",
            "Epoch 3/10, Loss: 7735.5150\n",
            "Accuracy: 0.7725\n",
            "Epoch 4/10, Loss: 6091.0452\n",
            "Accuracy: 0.7737\n",
            "Epoch 5/10, Loss: 5323.4264\n",
            "Accuracy: 0.7743\n",
            "Epoch 6/10, Loss: 4886.4637\n",
            "Accuracy: 0.7729\n",
            "Epoch 7/10, Loss: 4596.4812\n",
            "Accuracy: 0.7727\n",
            "Epoch 8/10, Loss: 4382.1922\n",
            "Accuracy: 0.7727\n",
            "Epoch 9/10, Loss: 4208.7536\n",
            "Accuracy: 0.7731\n",
            "Epoch 10/10, Loss: 4059.0412\n",
            "Accuracy: 0.7745\n",
            "Evaluating on DEVTEST set...\n",
            "Accuracy: 0.7894\n",
            "Epoch 1/10, Loss: 26248.9624\n",
            "Accuracy: 0.7264\n",
            "Epoch 2/10, Loss: 10571.9282\n",
            "Accuracy: 0.7930\n",
            "Epoch 3/10, Loss: 5715.2257\n",
            "Accuracy: 0.7953\n",
            "Epoch 4/10, Loss: 3615.7730\n",
            "Accuracy: 0.7955\n",
            "Epoch 5/10, Loss: 2390.1751\n",
            "Accuracy: 0.7944\n",
            "Epoch 6/10, Loss: 1653.6959\n",
            "Accuracy: 0.7994\n",
            "Epoch 7/10, Loss: 1250.9881\n",
            "Accuracy: 0.8007\n",
            "Epoch 8/10, Loss: 946.4691\n",
            "Accuracy: 0.8017\n",
            "Epoch 9/10, Loss: 769.2347\n",
            "Accuracy: 0.7957\n",
            "Epoch 10/10, Loss: 602.4454\n",
            "Accuracy: 0.7982\n",
            "Evaluating on DEVTEST set...\n",
            "Accuracy: 0.8176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2 Feature Engineering (15 points)\n",
        "\n",
        "Add features to the model by concatenating your own feature function outputs to the word embedding\n",
        "concatenation used above. Define feature functions based on looking at the training data, based on\n",
        "looking at the errors your tagger makes on DEV, or simply based on your intuitions about the task.\n",
        "For example, you could add binary features if the center word contains certain special characters or\n",
        "capitalization patterns, a feature that returns the number of characters in the center word, features\n",
        "for particular prefixes, suffixes, and other character patterns in the center word, etc. These sorts of\n",
        "features could also be defined for context words. You may find it helpful to use the orig-* files when\n",
        "computing features. (You will probably still want to use the twpos-* files for the word embeddings,\n",
        "though.)\n",
        "Develop and experiment with features and describe your results. You should be able to improve upon\n",
        "the accuracies you were seeing in Section 1.1.\n"
      ],
      "metadata": {
        "id": "tdJNfs9C_ixq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Code for feature extraction for a single word\n",
        "def extract_features(word):\n",
        "\n",
        "    capitalization = 1 if word[0].isupper() else 0\n",
        "\n",
        "    special_char = 1 if any(not char.isalnum() for char in word) else 0\n",
        "\n",
        "    word_length = len(word) / 10.0\n",
        "    prefix = word[:3]\n",
        "    suffix = word[-3:]\n",
        "    prefix_idx = sum(ord(c) for c in prefix) % 1000\n",
        "    suffix_idx = sum(ord(c) for c in suffix) % 1000\n",
        "    prefix_norm = prefix_idx / 1000.0\n",
        "    suffix_norm = suffix_idx / 1000.0\n",
        "\n",
        "    return torch.tensor([capitalization, special_char, word_length, prefix_norm, suffix_norm], dtype=torch.float)\n",
        "\n",
        "# Code to create create input vector by concatenating word embeddings with features\n",
        "def get_input_vector(context, word_to_idx, embedding_layer):\n",
        "    context_embeddings = []\n",
        "    context_features = []\n",
        "\n",
        "    for word in context:\n",
        "        word_idx = get_word_index(word, word_to_idx)\n",
        "        word_embedding = embedding_layer(torch.tensor([word_idx], dtype=torch.long)).squeeze(0)\n",
        "\n",
        "        word_features = extract_features(word)\n",
        "\n",
        "        combined_vector = torch.cat((word_embedding, word_features), dim=0)\n",
        "        context_embeddings.append(combined_vector)\n",
        "\n",
        "    return torch.cat(context_embeddings).view(1, -1)\n",
        "\n",
        "# Define the model class for Feed Forward N.N. by adding feature dimension too\n",
        "class POSModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, feature_dim, hidden_dim, output_dim, window_size):\n",
        "        super(POSModel, self).__init__()\n",
        "        self.embedding = initialize_embeddings(vocab_size, embedding_dim)\n",
        "\n",
        "        input_dim = (embedding_dim + feature_dim) * (2 * window_size + 1)\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tanh(self.fc1(x))\n",
        "        scores = self.fc2(x)\n",
        "        return scores\n",
        "\n",
        "# Code for training the Model\n",
        "def train_model(model, train_data, dev_data, word_to_idx, pos_dict, window_size, epochs=10, lr=0.02):\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "\n",
        "        for tweet in train_data:\n",
        "            for i, (word, pos) in enumerate(tweet):\n",
        "                context = get_context(tweet, i, window_size)\n",
        "                input_vector = get_input_vector(context, word_to_idx, model.embedding)\n",
        "                target = torch.tensor([pos_dict[pos]], dtype=torch.long)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                output = model(input_vector)\n",
        "\n",
        "                loss = criterion(output, target)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}')\n",
        "        evaluate_model(model, dev_data, word_to_idx, pos_dict, window_size, 4821)\n",
        "\n",
        "\n",
        "# Code for evaluating the Model\n",
        "def evaluate_model(model, test_data, word_to_idx, pos_dict, window_size, expected_count):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for tweet in test_data:\n",
        "            for i, (word, pos) in enumerate(tweet):\n",
        "                context = get_context(tweet, i, window_size)\n",
        "                input_vector = get_input_vector(context, word_to_idx, model.embedding)\n",
        "                target = pos_dict[pos]\n",
        "\n",
        "                output = model(input_vector)\n",
        "                predicted = torch.argmax(output).item()\n",
        "                if predicted == target:\n",
        "                    correct += 1\n",
        "                total += 1\n",
        "\n",
        "    assert total == expected_count, f\"Expected {expected_count} predictions, but got {total}\"\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "    return accuracy\n",
        "\n",
        "# A utility function to read files and setting dimensions and then calling the model to run on this data\n",
        "def result(w):\n",
        "    vocab = set()\n",
        "    train_data = load_data('/twpos-train.tsv')\n",
        "    dev_data = load_data('/twpos-dev.tsv')\n",
        "    devtest_data = load_data('/twpos-devtest.tsv')\n",
        "\n",
        "    for tweet in train_data:\n",
        "        for word, _ in tweet:\n",
        "            vocab.add(word)\n",
        "\n",
        "    word_to_idx = {word: idx for idx, word in enumerate(vocab, start=1)}\n",
        "    word_to_idx['<s>'] = 0\n",
        "    word_to_idx['</s>'] = 0\n",
        "    word_to_idx['UUUNKKK'] = len(vocab) + 1\n",
        "\n",
        "    pos_dict = pos_to_index()\n",
        "\n",
        "    vocab_size = len(word_to_idx)\n",
        "    embedding_dim = 50\n",
        "    feature_dim = 5\n",
        "    hidden_dim = 128\n",
        "    output_dim = len(pos_dict)\n",
        "    window_size = w\n",
        "\n",
        "    model = POSModel(vocab_size, embedding_dim, feature_dim, hidden_dim, output_dim, window_size)\n",
        "\n",
        "    model.embedding.weight.data[word_to_idx['UUUNKKK']] = 0\n",
        "    model.embedding.weight.data[word_to_idx['<s>']] = 0\n",
        "\n",
        "    # Code to call Train and evaluate function for the model\n",
        "    train_model(model, train_data, dev_data, word_to_idx, pos_dict, window_size, epochs=10)\n",
        "    print(\"Evaluating on DEVTEST set...\")\n",
        "    evaluate_model(model, devtest_data, word_to_idx, pos_dict, window_size, 4639)\n",
        "\n",
        "def main():\n",
        "    context_window = [0, 1]\n",
        "    for w in context_window:\n",
        "        result(w)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_GH2CbP_5f5",
        "outputId": "7239a9f5-2676-4be8-ce61-d322b0c5b19c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 22638.2898\n",
            "Accuracy: 0.7185\n",
            "Epoch 2/10, Loss: 11121.6414\n",
            "Accuracy: 0.7741\n",
            "Epoch 3/10, Loss: 7399.7439\n",
            "Accuracy: 0.7770\n",
            "Epoch 4/10, Loss: 5998.6403\n",
            "Accuracy: 0.7795\n",
            "Epoch 5/10, Loss: 5255.0829\n",
            "Accuracy: 0.7793\n",
            "Epoch 6/10, Loss: 4798.8504\n",
            "Accuracy: 0.7785\n",
            "Epoch 7/10, Loss: 4493.9533\n",
            "Accuracy: 0.7785\n",
            "Epoch 8/10, Loss: 4269.9913\n",
            "Accuracy: 0.7791\n",
            "Epoch 9/10, Loss: 4094.4390\n",
            "Accuracy: 0.7787\n",
            "Epoch 10/10, Loss: 3947.8282\n",
            "Accuracy: 0.7787\n",
            "Evaluating on DEVTEST set...\n",
            "Accuracy: 0.7948\n",
            "Epoch 1/10, Loss: 21479.1382\n",
            "Accuracy: 0.7434\n",
            "Epoch 2/10, Loss: 9435.1631\n",
            "Accuracy: 0.8048\n",
            "Epoch 3/10, Loss: 5385.6582\n",
            "Accuracy: 0.8079\n",
            "Epoch 4/10, Loss: 3513.5458\n",
            "Accuracy: 0.8119\n",
            "Epoch 5/10, Loss: 2397.7036\n",
            "Accuracy: 0.8191\n",
            "Epoch 6/10, Loss: 1657.3680\n",
            "Accuracy: 0.8189\n",
            "Epoch 7/10, Loss: 1181.6974\n",
            "Accuracy: 0.8177\n",
            "Epoch 8/10, Loss: 904.4941\n",
            "Accuracy: 0.8166\n",
            "Epoch 9/10, Loss: 724.6088\n",
            "Accuracy: 0.8171\n",
            "Epoch 10/10, Loss: 609.8508\n",
            "Accuracy: 0.8154\n",
            "Evaluating on DEVTEST set...\n",
            "Accuracy: 0.8263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3 Pretrained Embeddings (10 points)\n",
        "Initialize your word embeddings using the pretrained embeddings from twitter-embeddings.txt.\n",
        "For words in the tagging datasets that are not in the pretrained embeddings, use the unknown word\n",
        "embedding (i.e., the embedding for the word “UUUNKKK”). The pretrained embeddings contain an\n",
        "embedding for the sentence end symbol </s>, but not the sentence start symbol.\n",
        "\n",
        "1.3.1 Experiment with updating (fine-tuning) the pretrained embeddings for both w = 0 and w = 1 and\n",
        "report your results. You should see improvements over the randomly-initialized word embedding\n",
        "experiments from Section 1.1."
      ],
      "metadata": {
        "id": "g8n8Wdy9-pDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# Code to load word embeddings from file containing pretrained embeddings\n",
        "def load_word_embeddings(twitter_file):\n",
        "    embeddings = {}\n",
        "    with open(twitter_file, 'r') as f:\n",
        "        for line in f:\n",
        "            values = line.strip().split()\n",
        "            word = values[0]\n",
        "            word_vec = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings[word] = word_vec\n",
        "    embeddings['<s>'] = np.zeros(50)\n",
        "    embeddings['</s>'] = np.zeros(50)\n",
        "    return embeddings\n",
        "\n",
        "# COde for getting embedding vector for a word\n",
        "def get_embedding_vector(word, embeddings):\n",
        "    return torch.tensor(embeddings.get(word, embeddings['UUUNKKK']), dtype=torch.float32)\n",
        "\n",
        "# Code for creating input vector by concatenating word embeddings in the context window\n",
        "def get_input_vector(context, embeddings):\n",
        "    return np.concatenate([get_embedding_vector(word, embeddings) for word in context])"
      ],
      "metadata": {
        "id": "N-Y3VaNTM0xG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Feed Forward N.N. Model for Pretrained Embeddings\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "     def __init__(self, embedding_dim, hidden_dim, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "     def forward(self, x):\n",
        "        x = self.tanh(self.fc1(x))\n",
        "        scores = self.fc2(x)\n",
        "        return scores"
      ],
      "metadata": {
        "id": "AW2B5kALx9Im"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Code for training the Model\n",
        "def train_model(model, train_data, dev_data, embeddings, pos_dict, window_size, epochs=10, lr=0.02):\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "\n",
        "        for tweet in train_data:\n",
        "            for i, (word, pos) in enumerate(tweet):\n",
        "                context = get_context(tweet, i, window_size)\n",
        "                input_vector = torch.tensor(get_input_vector(context, embeddings), dtype=torch.float32)\n",
        "                target = torch.tensor([pos_dict[pos]], dtype=torch.long)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                output = model(input_vector)\n",
        "                loss = criterion(output.unsqueeze(0), target)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}')\n",
        "        evaluate_model(model, dev_data, embeddings, pos_dict, window_size, 4821)\n",
        "\n",
        "# Code for evaluating the Model\n",
        "def evaluate_model(model, test_data, embeddings, pos_dict, window_size, expected_count):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for tweet in test_data:\n",
        "            for i, (word, pos) in enumerate(tweet):\n",
        "                context = get_context(tweet, i, window_size)\n",
        "                input_vector = torch.tensor(get_input_vector(context, embeddings), dtype=torch.float32)\n",
        "                target = pos_dict[pos]\n",
        "\n",
        "                output = model(input_vector)\n",
        "                predicted = torch.argmax(output).item()\n",
        "                if predicted == target:\n",
        "                    correct += 1\n",
        "                total += 1\n",
        "\n",
        "    assert total == expected_count, f\"Expected {expected_count} predictions, but got {total}\"\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "Y_5dLR0_0I8q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Utility Funtion\n",
        "def result(w):\n",
        "    embeddings = load_word_embeddings('/twitter-embeddings.txt')\n",
        "\n",
        "    train_data = load_data('/twpos-train.tsv')\n",
        "    dev_data = load_data('/twpos-dev.tsv')\n",
        "    devtest_data = load_data('/twpos-devtest.tsv')\n",
        "\n",
        "    pos_dict = pos_to_index()\n",
        "\n",
        "    input_dim = 50 * (2*w + 1)\n",
        "    hidden_dim = 128\n",
        "    output_dim = len(pos_dict)\n",
        "    window_size = w\n",
        "\n",
        "    model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "    train_model(model, train_data, dev_data, embeddings, pos_dict, window_size, epochs=10)\n",
        "    print(\"Evaluating on DEVTEST set...\")\n",
        "    evaluate_model(model, devtest_data, embeddings, pos_dict, window_size, 4639)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "U39KapYkzaeG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    context_window = [0, 1]\n",
        "    for w in context_window:\n",
        "        result(w)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "m-yAj2T33PHI",
        "outputId": "ec70f1ca-e732-4a20-eb8c-4874000332d0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 14436.3236\n",
            "Accuracy: 0.8102\n",
            "Epoch 2/10, Loss: 10423.5516\n",
            "Accuracy: 0.8168\n",
            "Epoch 3/10, Loss: 10012.9584\n",
            "Accuracy: 0.8177\n",
            "Epoch 4/10, Loss: 9749.0976\n",
            "Accuracy: 0.8195\n",
            "Epoch 5/10, Loss: 9518.9686\n",
            "Accuracy: 0.8200\n",
            "Epoch 6/10, Loss: 9292.5743\n",
            "Accuracy: 0.8245\n",
            "Epoch 7/10, Loss: 9071.1312\n",
            "Accuracy: 0.8247\n",
            "Epoch 8/10, Loss: 8866.4081\n",
            "Accuracy: 0.8224\n",
            "Epoch 9/10, Loss: 8685.2950\n",
            "Accuracy: 0.8222\n",
            "Epoch 10/10, Loss: 8527.8389\n",
            "Accuracy: 0.8245\n",
            "Evaluating on DEVTEST set...\n",
            "Accuracy: 0.8230\n",
            "Epoch 1/10, Loss: 13674.0126\n",
            "Accuracy: 0.8243\n",
            "Epoch 2/10, Loss: 8810.1603\n",
            "Accuracy: 0.8345\n",
            "Epoch 3/10, Loss: 8107.2609\n",
            "Accuracy: 0.8351\n",
            "Epoch 4/10, Loss: 7649.6606\n",
            "Accuracy: 0.8355\n",
            "Epoch 5/10, Loss: 7267.2322\n",
            "Accuracy: 0.8365\n",
            "Epoch 6/10, Loss: 6925.3962\n",
            "Accuracy: 0.8403\n",
            "Epoch 7/10, Loss: 6609.0819\n",
            "Accuracy: 0.8428\n",
            "Epoch 8/10, Loss: 6309.7090\n",
            "Accuracy: 0.8471\n",
            "Epoch 9/10, Loss: 6020.9735\n",
            "Accuracy: 0.8488\n",
            "Epoch 10/10, Loss: 5737.4976\n",
            "Accuracy: 0.8490\n",
            "Evaluating on DEVTEST set...\n",
            "Accuracy: 0.8534\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3.2  \n",
        "With w = 1, empirically compare updating the pretrained word embeddings during training and\n",
        "keeping them fixed."
      ],
      "metadata": {
        "id": "L55rLzxEqXZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Code for training the model by keeping embeddings fixed and updating\n",
        "def train_model(model, train_data, dev_data, embeddings, pos_dict, window_size, epochs=10, lr=0.02, update_embeddings=True):\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "\n",
        "        for tweet in train_data:\n",
        "            for i, (word, pos) in enumerate(tweet):\n",
        "                context = get_context(tweet, i, window_size)\n",
        "                input_vector = torch.tensor(get_input_vector(context, embeddings), dtype=torch.float32)\n",
        "\n",
        "                if update_embeddings:\n",
        "                    input_vector.requires_grad = True\n",
        "                else:\n",
        "                    input_vector.requires_grad = False\n",
        "\n",
        "                target = torch.tensor([pos_dict[pos]], dtype=torch.long)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                output = model(input_vector)\n",
        "\n",
        "                loss = criterion(output.unsqueeze(0), target)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}')\n",
        "        evaluate_model(model, dev_data, embeddings, pos_dict, window_size, 4821)\n",
        "\n",
        "\n",
        "# Utility Function\n",
        "def result(w, update_embeddings):\n",
        "    embeddings = load_word_embeddings('/twitter-embeddings.txt')\n",
        "\n",
        "    train_data = load_data('/twpos-train.tsv')\n",
        "    dev_data = load_data('/twpos-dev.tsv')\n",
        "    devtest_data = load_data('/twpos-devtest.tsv')\n",
        "\n",
        "    pos_dict = pos_to_index()\n",
        "\n",
        "    input_dim = 50 * (2 * w + 1)\n",
        "    hidden_dim = 128\n",
        "    output_dim = len(pos_dict)\n",
        "\n",
        "    model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "    print(f\"\\nTraining with {'updating' if update_embeddings else 'fixed'} embeddings:\")\n",
        "    train_model(model, train_data, dev_data, embeddings, pos_dict, w, epochs=10, update_embeddings=update_embeddings)\n",
        "    print(\"Evaluating on DEVTEST set...\")\n",
        "    evaluate_model(model, devtest_data, embeddings, pos_dict, w, 4639)\n",
        "\n",
        "def main():\n",
        "    context_window = [1]\n",
        "    for w in context_window:\n",
        "        result(w, update_embeddings=True)\n",
        "        result(w, update_embeddings=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C02soA3NiDl3",
        "outputId": "502c4dbe-9740-4224-db64-8deb74dc667d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with updating embeddings:\n",
            "Epoch 1/10, Loss: 13640.3876\n",
            "Accuracy: 0.8251\n",
            "Epoch 2/10, Loss: 8768.6606\n",
            "Accuracy: 0.8330\n",
            "Epoch 3/10, Loss: 8081.7865\n",
            "Accuracy: 0.8359\n",
            "Epoch 4/10, Loss: 7640.4794\n",
            "Accuracy: 0.8376\n",
            "Epoch 5/10, Loss: 7272.5088\n",
            "Accuracy: 0.8365\n",
            "Epoch 6/10, Loss: 6929.9067\n",
            "Accuracy: 0.8390\n",
            "Epoch 7/10, Loss: 6594.4760\n",
            "Accuracy: 0.8411\n",
            "Epoch 8/10, Loss: 6262.6499\n",
            "Accuracy: 0.8434\n",
            "Epoch 9/10, Loss: 5936.6733\n",
            "Accuracy: 0.8477\n",
            "Epoch 10/10, Loss: 5619.5313\n",
            "Accuracy: 0.8494\n",
            "Evaluating on DEVTEST set...\n",
            "Accuracy: 0.8467\n",
            "\n",
            "Training with fixed embeddings:\n",
            "Epoch 1/10, Loss: 13596.9028\n",
            "Accuracy: 0.8251\n",
            "Epoch 2/10, Loss: 8778.0269\n",
            "Accuracy: 0.8324\n",
            "Epoch 3/10, Loss: 8111.7985\n",
            "Accuracy: 0.8343\n",
            "Epoch 4/10, Loss: 7687.1712\n",
            "Accuracy: 0.8343\n",
            "Epoch 5/10, Loss: 7327.7407\n",
            "Accuracy: 0.8357\n",
            "Epoch 6/10, Loss: 6988.6198\n",
            "Accuracy: 0.8365\n",
            "Epoch 7/10, Loss: 6654.0582\n",
            "Accuracy: 0.8401\n",
            "Epoch 8/10, Loss: 6325.0811\n",
            "Accuracy: 0.8446\n",
            "Epoch 9/10, Loss: 6003.6220\n",
            "Accuracy: 0.8471\n",
            "Epoch 10/10, Loss: 5690.7811\n",
            "Accuracy: 0.8463\n",
            "Evaluating on DEVTEST set...\n",
            "Accuracy: 0.8506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3.3\n",
        "\n",
        "Combine your features from Section 1.2 with the use of pretrained embeddings. Do the features\n",
        "still help?"
      ],
      "metadata": {
        "id": "zhbahTNeqeJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# Code to get embedding vector for a word\n",
        "def get_embedding_vector(word, embeddings):\n",
        "    return torch.tensor(embeddings.get(word, embeddings['UUUNKKK']), dtype=torch.float32)\n",
        "\n",
        "# Code to normalize features to be in the same range as word embeddings\n",
        "def extract_features(word):\n",
        "    features = []\n",
        "    features.append(1 if word[0].isupper() else 0)\n",
        "\n",
        "    features.append(len(word) / 10.0)\n",
        "    features.append(1 if \"@\" in word or \"#\" in word else 0)\n",
        "    prefix = word[:2]\n",
        "    suffix = word[-2:]\n",
        "    features.append((hash(prefix) % 100) / 100.0)\n",
        "    features.append((hash(suffix) % 100) / 100.0)\n",
        "\n",
        "    return np.array(features, dtype='float32')\n",
        "\n",
        "# Code to create input vector by concatenating word embeddings and normalized additional features\n",
        "def get_input_vector(context, embeddings):\n",
        "    embedding_vector = np.concatenate([get_embedding_vector(word, embeddings) for word in context])\n",
        "    features = extract_features(context[len(context)//2])\n",
        "    return np.concatenate([embedding_vector, features])\n"
      ],
      "metadata": {
        "id": "4evpqI6DI1gz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Code to implement Model\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "     def __init__(self, embedding_dim, feature_dim, hidden_dim, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        input_dim = embedding_dim + feature_dim\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "     def forward(self, x):\n",
        "        x = self.tanh(self.fc1(x))\n",
        "        scores = self.fc2(x)\n",
        "        return scores"
      ],
      "metadata": {
        "id": "Z1Ylh57DInzW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility Function\n",
        "def result(w):\n",
        "    embeddings = load_word_embeddings('/twitter-embeddings.txt')\n",
        "    train_data = load_data('/twpos-train.tsv')\n",
        "    dev_data = load_data('/twpos-dev.tsv')\n",
        "    devtest_data = load_data('/twpos-devtest.tsv')\n",
        "\n",
        "    pos_dict = pos_to_index()\n",
        "\n",
        "    input_dim = 50 * (2*w + 1)\n",
        "    feature_dim = 5\n",
        "    hidden_dim = 128\n",
        "    output_dim = len(pos_dict)\n",
        "    window_size = w\n",
        "\n",
        "    model = FeedforwardNeuralNetModel(input_dim, feature_dim, hidden_dim, output_dim)\n",
        "\n",
        "    train_model(model, train_data, dev_data, embeddings, pos_dict, window_size, epochs=10)\n",
        "    print(\"Evaluating on DEVTEST set...\")\n",
        "    evaluate_model(model, devtest_data, embeddings, pos_dict, window_size, 4639)"
      ],
      "metadata": {
        "id": "W4TYoKwsIyu5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    context_window = [0, 1]\n",
        "    for w in context_window:\n",
        "        result(w)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mt58OivZJj_o",
        "outputId": "cef9c628-d66a-443b-a678-94f6ac08d802"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 13832.6978\n",
            "Accuracy: 0.8197\n",
            "Epoch 2/10, Loss: 9782.1717\n",
            "Accuracy: 0.8227\n",
            "Epoch 3/10, Loss: 9373.8821\n",
            "Accuracy: 0.8243\n",
            "Epoch 4/10, Loss: 9122.7273\n",
            "Accuracy: 0.8256\n",
            "Epoch 5/10, Loss: 8911.2973\n",
            "Accuracy: 0.8278\n",
            "Epoch 6/10, Loss: 8711.0557\n",
            "Accuracy: 0.8270\n",
            "Epoch 7/10, Loss: 8516.9677\n",
            "Accuracy: 0.8283\n",
            "Epoch 8/10, Loss: 8332.3968\n",
            "Accuracy: 0.8305\n",
            "Epoch 9/10, Loss: 8162.2448\n",
            "Accuracy: 0.8314\n",
            "Epoch 10/10, Loss: 8008.4303\n",
            "Accuracy: 0.8349\n",
            "Evaluating on DEVTEST set...\n",
            "Accuracy: 0.8319\n",
            "Epoch 1/10, Loss: 12976.2816\n",
            "Accuracy: 0.8293\n",
            "Epoch 2/10, Loss: 8230.7429\n",
            "Accuracy: 0.8405\n",
            "Epoch 3/10, Loss: 7572.0660\n",
            "Accuracy: 0.8426\n",
            "Epoch 4/10, Loss: 7149.6051\n",
            "Accuracy: 0.8444\n",
            "Epoch 5/10, Loss: 6799.0688\n",
            "Accuracy: 0.8461\n",
            "Epoch 6/10, Loss: 6477.5862\n",
            "Accuracy: 0.8475\n",
            "Epoch 7/10, Loss: 6168.9701\n",
            "Accuracy: 0.8490\n",
            "Epoch 8/10, Loss: 5866.2953\n",
            "Accuracy: 0.8525\n",
            "Epoch 9/10, Loss: 5566.7842\n",
            "Accuracy: 0.8544\n",
            "Epoch 10/10, Loss: 5270.8547\n",
            "Accuracy: 0.8583\n",
            "Evaluating on DEVTEST set...\n",
            "Accuracy: 0.8629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.4\n",
        "\n",
        "Architecture Engineering (10 points)\n",
        "Using the best configuration from above, explore the space of neural architectures to see if you can\n",
        "improve your tagger further. Some suggestions are below:\n",
        "• Compare the use of 0, 1, and 2 hidden layers. For each number of hidden layers, try two different\n",
        "layer widths that differ by a factor of 2 (e.g., 256 and 512).\n",
        "• Keeping the number of layers and layer sizes fixed, experiment with different nonlinearities, e.g.,\n",
        "identity (g(a) = a), tanh, ReLU, and logistic sigmoid.\n",
        "• Experiment with w = 2 and compare the results to w = 0 and 1.\n"
      ],
      "metadata": {
        "id": "IsC3uS2XuKsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Class for N.N. model for different sctivation Functions\n",
        "class DynamicFeedforwardModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_layers, layer_widths, activation_function, output_dim):\n",
        "        super(DynamicFeedforwardModel, self).__init__()\n",
        "\n",
        "        # Initialize layers\n",
        "        layers = []\n",
        "        current_input_dim = input_dim\n",
        "\n",
        "        for i in range(hidden_layers):\n",
        "            layers.append(nn.Linear(current_input_dim, layer_widths[i]))\n",
        "            if activation_function == 'tanh':\n",
        "                layers.append(nn.Tanh())\n",
        "            elif activation_function == 'relu':\n",
        "                layers.append(nn.ReLU())\n",
        "            elif activation_function == 'sigmoid':\n",
        "                layers.append(nn.Sigmoid())\n",
        "            elif activation_function == 'identity':\n",
        "                layers.append(nn.Identity())\n",
        "            current_input_dim = layer_widths[i]\n",
        "\n",
        "        layers.append(nn.Linear(current_input_dim, output_dim))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Code for training the model\n",
        "def train_model(model, train_data, dev_data, embeddings, pos_dict, window_size, epochs=10, lr=0.02, update_embeddings=True):\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "\n",
        "        for tweet in train_data:\n",
        "            for i, (word, pos) in enumerate(tweet):\n",
        "                context = get_context(tweet, i, window_size)\n",
        "                input_vector = torch.tensor(get_input_vector(context, embeddings), dtype=torch.float32)\n",
        "\n",
        "                if update_embeddings:\n",
        "                    input_vector.requires_grad = True\n",
        "                else:\n",
        "                    input_vector.requires_grad = False\n",
        "\n",
        "                target = torch.tensor([pos_dict[pos]], dtype=torch.long)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                output = model(input_vector)\n",
        "                loss = criterion(output.unsqueeze(0), target)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}')\n",
        "        evaluate_model(model, dev_data, embeddings, pos_dict, window_size, 4821)\n",
        "\n",
        "# Code for evaluating the Model\n",
        "def evaluate_model(model, test_data, embeddings, pos_dict, window_size, expected_count):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for tweet in test_data:\n",
        "            for i, (word, pos) in enumerate(tweet):\n",
        "                context = get_context(tweet, i, window_size)\n",
        "                input_vector = torch.tensor(get_input_vector(context, embeddings), dtype=torch.float32)\n",
        "                target = pos_dict[pos]\n",
        "                output = model(input_vector)\n",
        "                predicted = torch.argmax(output).item()\n",
        "                if predicted == target:\n",
        "                    correct += 1\n",
        "                total += 1\n",
        "\n",
        "    assert total == expected_count, f\"Expected {expected_count} predictions, but got {total}\"\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "    return accuracy\n",
        "\n",
        "# Function to run different experiments by changing the hidden layers and activation functions\n",
        "def run_experiments():\n",
        "    embeddings = load_word_embeddings('/twitter-embeddings.txt')\n",
        "    train_data = load_data('/twpos-train.tsv')\n",
        "    dev_data = load_data('/twpos-dev.tsv')\n",
        "    devtest_data = load_data('/twpos-devtest.tsv')\n",
        "    pos_dict = pos_to_index()\n",
        "\n",
        "    window_sizes = [0, 1, 2]\n",
        "    layer_combinations = [\n",
        "        (0, [128]),  # 0 hidden layer\n",
        "        (1, [128]),  # 1 hidden layer\n",
        "        (1, [256]),  # 1 hidden layer\n",
        "        (1, [512]),  # 1 hidden layer\n",
        "        (2, [128, 256]),  # 2 hidden layers\n",
        "        (2, [256, 512]),  # 2 hidden layers\n",
        "    ]\n",
        "    activations = ['tanh', 'relu', 'sigmoid', 'identity']\n",
        "    epochs = 10\n",
        "\n",
        "    for window_size in window_sizes:\n",
        "        for hidden_layers, layer_widths in layer_combinations:\n",
        "            for activation in activations:\n",
        "                input_dim = 50 * (2 * window_size + 1)\n",
        "                output_dim = len(pos_dict)\n",
        "\n",
        "                print(f\"\\nRunning experiment with window_size={window_size}, hidden_layers={hidden_layers}, \"\n",
        "                      f\"layer_widths={layer_widths}, activation={activation}\")\n",
        "\n",
        "                model = DynamicFeedforwardModel(input_dim, hidden_layers, layer_widths, activation, output_dim)\n",
        "                train_model(model, train_data, dev_data, embeddings, pos_dict, window_size, epochs)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_experiments()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDaalfs1uMv4",
        "outputId": "69e9303b-75f7-47ea-a49b-33b0b208513a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running experiment with window_size=0, hidden_layers=0, layer_widths=[128], activation=tanh\n",
            "Epoch 1/10, Loss: 20067.9755\n",
            "Accuracy: 0.7764\n",
            "Epoch 2/10, Loss: 12808.6664\n",
            "Accuracy: 0.8075\n",
            "Epoch 3/10, Loss: 11624.6174\n",
            "Accuracy: 0.8129\n",
            "Epoch 4/10, Loss: 11073.4463\n",
            "Accuracy: 0.8175\n",
            "Epoch 5/10, Loss: 10740.1414\n",
            "Accuracy: 0.8181\n",
            "Epoch 6/10, Loss: 10511.0815\n",
            "Accuracy: 0.8214\n",
            "Epoch 7/10, Loss: 10341.3145\n",
            "Accuracy: 0.8210\n",
            "Epoch 8/10, Loss: 10209.0107\n",
            "Accuracy: 0.8214\n",
            "Epoch 9/10, Loss: 10102.1276\n",
            "Accuracy: 0.8212\n",
            "Epoch 10/10, Loss: 10013.4223\n",
            "Accuracy: 0.8239\n",
            "\n",
            "Running experiment with window_size=0, hidden_layers=0, layer_widths=[128], activation=relu\n",
            "Epoch 1/10, Loss: 20037.1366\n",
            "Accuracy: 0.7766\n",
            "Epoch 2/10, Loss: 12805.4272\n",
            "Accuracy: 0.8073\n",
            "Epoch 3/10, Loss: 11622.7861\n",
            "Accuracy: 0.8127\n",
            "Epoch 4/10, Loss: 11071.6904\n",
            "Accuracy: 0.8179\n",
            "Epoch 5/10, Loss: 10738.4446\n",
            "Accuracy: 0.8179\n",
            "Epoch 6/10, Loss: 10509.4469\n",
            "Accuracy: 0.8206\n",
            "Epoch 7/10, Loss: 10339.7156\n",
            "Accuracy: 0.8208\n",
            "Epoch 8/10, Loss: 10207.4179\n",
            "Accuracy: 0.8214\n",
            "Epoch 9/10, Loss: 10100.5181\n",
            "Accuracy: 0.8214\n",
            "Epoch 10/10, Loss: 10011.7867\n",
            "Accuracy: 0.8243\n",
            "\n",
            "Running experiment with window_size=0, hidden_layers=0, layer_widths=[128], activation=sigmoid\n",
            "Epoch 1/10, Loss: 19985.5570\n",
            "Accuracy: 0.7768\n",
            "Epoch 2/10, Loss: 12802.8071\n",
            "Accuracy: 0.8077\n",
            "Epoch 3/10, Loss: 11622.1811\n",
            "Accuracy: 0.8135\n",
            "Epoch 4/10, Loss: 11072.5509\n",
            "Accuracy: 0.8168\n",
            "Epoch 5/10, Loss: 10739.9986\n",
            "Accuracy: 0.8181\n",
            "Epoch 6/10, Loss: 10511.3348\n",
            "Accuracy: 0.8206\n",
            "Epoch 7/10, Loss: 10341.7706\n",
            "Accuracy: 0.8214\n",
            "Epoch 8/10, Loss: 10209.5696\n",
            "Accuracy: 0.8216\n",
            "Epoch 9/10, Loss: 10102.7365\n",
            "Accuracy: 0.8210\n",
            "Epoch 10/10, Loss: 10014.0555\n",
            "Accuracy: 0.8237\n",
            "\n",
            "Running experiment with window_size=0, hidden_layers=0, layer_widths=[128], activation=identity\n",
            "Epoch 1/10, Loss: 20036.1036\n",
            "Accuracy: 0.7776\n",
            "Epoch 2/10, Loss: 12815.9507\n",
            "Accuracy: 0.8065\n",
            "Epoch 3/10, Loss: 11627.1143\n",
            "Accuracy: 0.8131\n",
            "Epoch 4/10, Loss: 11074.2094\n",
            "Accuracy: 0.8175\n",
            "Epoch 5/10, Loss: 10739.8966\n",
            "Accuracy: 0.8177\n",
            "Epoch 6/10, Loss: 10510.2428\n",
            "Accuracy: 0.8210\n",
            "Epoch 7/10, Loss: 10340.1209\n",
            "Accuracy: 0.8210\n",
            "Epoch 8/10, Loss: 10207.6049\n",
            "Accuracy: 0.8212\n",
            "Epoch 9/10, Loss: 10100.5921\n",
            "Accuracy: 0.8212\n",
            "Epoch 10/10, Loss: 10011.8111\n",
            "Accuracy: 0.8239\n",
            "\n",
            "Running experiment with window_size=0, hidden_layers=1, layer_widths=[128], activation=tanh\n",
            "Epoch 1/10, Loss: 14760.3487\n",
            "Accuracy: 0.7953\n",
            "Epoch 2/10, Loss: 10553.2942\n",
            "Accuracy: 0.8042\n",
            "Epoch 3/10, Loss: 10089.5054\n",
            "Accuracy: 0.8063\n",
            "Epoch 4/10, Loss: 9798.4148\n",
            "Accuracy: 0.8081\n",
            "Epoch 5/10, Loss: 9553.8669\n",
            "Accuracy: 0.8100\n",
            "Epoch 6/10, Loss: 9330.6323\n",
            "Accuracy: 0.8133\n",
            "Epoch 7/10, Loss: 9126.7138\n",
            "Accuracy: 0.8177\n",
            "Epoch 8/10, Loss: 8941.4685\n",
            "Accuracy: 0.8191\n",
            "Epoch 9/10, Loss: 8773.2296\n",
            "Accuracy: 0.8187\n",
            "Epoch 10/10, Loss: 8621.0925\n",
            "Accuracy: 0.8187\n",
            "\n",
            "Running experiment with window_size=0, hidden_layers=1, layer_widths=[128], activation=relu\n",
            "Epoch 1/10, Loss: 15074.6760\n",
            "Accuracy: 0.8102\n",
            "Epoch 2/10, Loss: 9787.2184\n",
            "Accuracy: 0.8237\n",
            "Epoch 3/10, Loss: 8993.5459\n",
            "Accuracy: 0.8301\n",
            "Epoch 4/10, Loss: 8483.5991\n",
            "Accuracy: 0.8357\n",
            "Epoch 5/10, Loss: 8137.2013\n",
            "Accuracy: 0.8401\n",
            "Epoch 6/10, Loss: 7878.3842\n",
            "Accuracy: 0.8405\n",
            "Epoch 7/10, Loss: 7666.2922\n",
            "Accuracy: 0.8397\n",
            "Epoch 8/10, Loss: 7488.3234\n",
            "Accuracy: 0.8409\n",
            "Epoch 9/10, Loss: 7326.8300\n",
            "Accuracy: 0.8407\n",
            "Epoch 10/10, Loss: 7183.5146\n",
            "Accuracy: 0.8411\n",
            "\n",
            "Running experiment with window_size=0, hidden_layers=1, layer_widths=[128], activation=sigmoid\n",
            "Epoch 1/10, Loss: 26794.3498\n",
            "Accuracy: 0.7295\n",
            "Epoch 2/10, Loss: 13676.9611\n",
            "Accuracy: 0.7575\n",
            "Epoch 3/10, Loss: 11778.9328\n",
            "Accuracy: 0.7886\n",
            "Epoch 4/10, Loss: 10979.8898\n",
            "Accuracy: 0.8112\n",
            "Epoch 5/10, Loss: 10546.8859\n",
            "Accuracy: 0.8168\n",
            "Epoch 6/10, Loss: 10263.6371\n",
            "Accuracy: 0.8179\n",
            "Epoch 7/10, Loss: 10052.9431\n",
            "Accuracy: 0.8187\n",
            "Epoch 8/10, Loss: 9882.9833\n",
            "Accuracy: 0.8193\n",
            "Epoch 9/10, Loss: 9737.9635\n",
            "Accuracy: 0.8210\n",
            "Epoch 10/10, Loss: 9609.1610\n",
            "Accuracy: 0.8222\n",
            "\n",
            "Running experiment with window_size=0, hidden_layers=1, layer_widths=[128], activation=identity\n",
            "Epoch 1/10, Loss: 14712.9020\n",
            "Accuracy: 0.7949\n",
            "Epoch 2/10, Loss: 10863.7214\n",
            "Accuracy: 0.8013\n",
            "Epoch 3/10, Loss: 10519.1170\n",
            "Accuracy: 0.8042\n",
            "Epoch 4/10, Loss: 10343.6080\n",
            "Accuracy: 0.8034\n",
            "Epoch 5/10, Loss: 10234.0902\n",
            "Accuracy: 0.8032\n",
            "Epoch 6/10, Loss: 10162.0497\n",
            "Accuracy: 0.8019\n",
            "Epoch 7/10, Loss: 10108.1409\n",
            "Accuracy: 0.8023\n",
            "Epoch 8/10, Loss: 10070.8568\n",
            "Accuracy: 0.8021\n",
            "Epoch 9/10, Loss: 10038.7107\n",
            "Accuracy: 0.8015\n",
            "Epoch 10/10, Loss: 10014.9459\n",
            "Accuracy: 0.8019\n",
            "\n",
            "Running experiment with window_size=0, hidden_layers=1, layer_widths=[256], activation=tanh\n",
            "Epoch 1/10, Loss: 14162.9025\n",
            "Accuracy: 0.7986\n",
            "Epoch 2/10, Loss: 10680.1302\n",
            "Accuracy: 0.8034\n",
            "Epoch 3/10, Loss: 10280.7793\n",
            "Accuracy: 0.8050\n",
            "Epoch 4/10, Loss: 10045.7103\n",
            "Accuracy: 0.8050\n",
            "Epoch 5/10, Loss: 9874.7612\n",
            "Accuracy: 0.8056\n",
            "Epoch 6/10, Loss: 9732.2265\n",
            "Accuracy: 0.8063\n",
            "Epoch 7/10, Loss: 9599.0864\n",
            "Accuracy: 0.8067\n",
            "Epoch 8/10, Loss: 9465.7000\n",
            "Accuracy: 0.8061\n",
            "Epoch 9/10, Loss: 9328.0706\n",
            "Accuracy: 0.8071\n",
            "Epoch 10/10, Loss: 9186.4084\n",
            "Accuracy: 0.8077\n",
            "\n",
            "Running experiment with window_size=0, hidden_layers=1, layer_widths=[256], activation=relu\n",
            "Epoch 1/10, Loss: 14416.3691\n",
            "Accuracy: 0.8156\n",
            "Epoch 2/10, Loss: 9606.9205\n",
            "Accuracy: 0.8297\n",
            "Epoch 3/10, Loss: 8806.7875\n",
            "Accuracy: 0.8353\n",
            "Epoch 4/10, Loss: 8310.9747\n",
            "Accuracy: 0.8413\n",
            "Epoch 5/10, Loss: 7941.6206\n",
            "Accuracy: 0.8413\n",
            "Epoch 6/10, Loss: 7654.7220\n",
            "Accuracy: 0.8421\n",
            "Epoch 7/10, Loss: 7431.3309\n",
            "Accuracy: 0.8419\n",
            "Epoch 8/10, Loss: 7240.5955\n",
            "Accuracy: 0.8413\n",
            "Epoch 9/10, Loss: 7073.7178\n",
            "Accuracy: 0.8417\n",
            "Epoch 10/10, Loss: 6931.8570\n",
            "Accuracy: 0.8424\n",
            "\n",
            "Running experiment with window_size=0, hidden_layers=1, layer_widths=[256], activation=sigmoid\n",
            "Epoch 1/10, Loss: 26202.1579\n",
            "Accuracy: 0.7349\n",
            "Epoch 2/10, Loss: 13682.7214\n",
            "Accuracy: 0.7689\n",
            "Epoch 3/10, Loss: 11952.4122\n",
            "Accuracy: 0.7824\n",
            "Epoch 4/10, Loss: 11209.0426\n",
            "Accuracy: 0.8036\n",
            "Epoch 5/10, Loss: 10792.3519\n",
            "Accuracy: 0.8125\n",
            "Epoch 6/10, Loss: 10510.1245\n",
            "Accuracy: 0.8160\n",
            "Epoch 7/10, Loss: 10296.4494\n",
            "Accuracy: 0.8171\n",
            "Epoch 8/10, Loss: 10123.1856\n",
            "Accuracy: 0.8173\n",
            "Epoch 9/10, Loss: 9975.5568\n",
            "Accuracy: 0.8183\n",
            "Epoch 10/10, Loss: 9845.0556\n",
            "Accuracy: 0.8195\n",
            "\n",
            "Running experiment with window_size=0, hidden_layers=1, layer_widths=[256], activation=identity\n",
            "Epoch 1/10, Loss: 14120.0923\n",
            "Accuracy: 0.7965\n",
            "Epoch 2/10, Loss: 10815.7520\n",
            "Accuracy: 0.8015\n",
            "Epoch 3/10, Loss: 10496.5661\n",
            "Accuracy: 0.8021\n",
            "Epoch 4/10, Loss: 10333.7273\n",
            "Accuracy: 0.8027\n",
            "Epoch 5/10, Loss: 10230.5314\n",
            "Accuracy: 0.8021\n",
            "Epoch 6/10, Loss: 10163.5514\n",
            "Accuracy: 0.8013\n",
            "Epoch 7/10, Loss: 10111.6441\n",
            "Accuracy: 0.8017\n",
            "Epoch 8/10, Loss: 10074.5138\n",
            "Accuracy: 0.8019\n",
            "Epoch 9/10, Loss: 10042.5901\n",
            "Accuracy: 0.8017\n",
            "Epoch 10/10, Loss: 10020.1842\n",
            "Accuracy: 0.8021\n",
            "\n",
            "Running experiment with window_size=0, hidden_layers=1, layer_widths=[512], activation=tanh\n",
            "Epoch 1/10, Loss: 13495.2746\n",
            "Accuracy: 0.7944\n",
            "Epoch 2/10, Loss: 10708.4203\n",
            "Accuracy: 0.8011\n",
            "Epoch 3/10, Loss: 10388.0098\n",
            "Accuracy: 0.8021\n",
            "Epoch 4/10, Loss: 10199.4085\n",
            "Accuracy: 0.8025\n",
            "Epoch 5/10, Loss: 10060.3103\n",
            "Accuracy: 0.8017\n",
            "Epoch 6/10, Loss: 9946.1572\n",
            "Accuracy: 0.8019\n",
            "Epoch 7/10, Loss: 9842.7878\n",
            "Accuracy: 0.8036\n",
            "Epoch 8/10, Loss: 9740.6961\n",
            "Accuracy: 0.8036\n",
            "Epoch 9/10, Loss: 9637.2745\n",
            "Accuracy: 0.8044\n",
            "Epoch 10/10, Loss: 9533.9372\n",
            "Accuracy: 0.8081\n",
            "\n",
            "Running experiment with window_size=0, hidden_layers=1, layer_widths=[512], activation=relu\n",
            "Epoch 1/10, Loss: 13711.2448\n",
            "Accuracy: 0.8171\n",
            "Epoch 2/10, Loss: 9432.5469\n",
            "Accuracy: 0.8312\n",
            "Epoch 3/10, Loss: 8628.9601\n",
            "Accuracy: 0.8368\n",
            "Epoch 4/10, Loss: 8128.6149\n",
            "Accuracy: 0.8407\n",
            "Epoch 5/10, Loss: 7771.6171\n",
            "Accuracy: 0.8428\n",
            "Epoch 6/10, Loss: 7493.6364\n",
            "Accuracy: 0.8432\n",
            "Epoch 7/10, Loss: 7274.7580\n",
            "Accuracy: 0.8440\n",
            "Epoch 8/10, Loss: 7079.1563\n",
            "Accuracy: 0.8434\n",
            "Epoch 9/10, Loss: 6912.9441\n",
            "Accuracy: 0.8432\n",
            "Epoch 10/10, Loss: 6772.2932\n",
            "Accuracy: 0.8419\n",
            "\n",
            "Running experiment with window_size=0, hidden_layers=1, layer_widths=[512], activation=sigmoid\n",
            "Epoch 1/10, Loss: 26588.1829\n",
            "Accuracy: 0.7343\n",
            "Epoch 2/10, Loss: 14298.8114\n",
            "Accuracy: 0.7501\n",
            "Epoch 3/10, Loss: 12591.4170\n",
            "Accuracy: 0.7791\n",
            "Epoch 4/10, Loss: 11827.2160\n",
            "Accuracy: 0.7957\n",
            "Epoch 5/10, Loss: 11362.7397\n",
            "Accuracy: 0.7955\n",
            "Epoch 6/10, Loss: 11030.3917\n",
            "Accuracy: 0.8034\n",
            "Epoch 7/10, Loss: 10772.6101\n",
            "Accuracy: 0.8067\n",
            "Epoch 8/10, Loss: 10562.4013\n",
            "Accuracy: 0.8102\n",
            "Epoch 9/10, Loss: 10384.5656\n",
            "Accuracy: 0.8117\n",
            "Epoch 10/10, Loss: 10229.9285\n",
            "Accuracy: 0.8125\n",
            "\n",
            "Running experiment with window_size=0, hidden_layers=1, layer_widths=[512], activation=identity\n",
            "Epoch 1/10, Loss: 13456.4171\n",
            "Accuracy: 0.7942\n",
            "Epoch 2/10, Loss: 10776.9319\n",
            "Accuracy: 0.7984\n",
            "Epoch 3/10, Loss: 10488.1364\n",
            "Accuracy: 0.8011\n",
            "Epoch 4/10, Loss: 10336.5897\n",
            "Accuracy: 0.8025\n",
            "Epoch 5/10, Loss: 10240.6720\n",
            "Accuracy: 0.8021\n",
            "Epoch 6/10, Loss: 10174.2137\n",
            "Accuracy: 0.8015\n",
            "Epoch 7/10, Loss: 10124.7467\n",
            "Accuracy: 0.8019\n",
            "Epoch 8/10, Loss: 10087.4569\n",
            "Accuracy: 0.8019\n",
            "Epoch 9/10, Loss: 10057.1184\n",
            "Accuracy: 0.8023\n",
            "Epoch 10/10, Loss: 10034.6585\n",
            "Accuracy: 0.8029\n",
            "\n",
            "Running experiment with window_size=0, hidden_layers=2, layer_widths=[128, 256], activation=tanh\n",
            "Epoch 1/10, Loss: 14360.2499\n",
            "Accuracy: 0.7874\n",
            "Epoch 2/10, Loss: 10888.3708\n",
            "Accuracy: 0.7955\n",
            "Epoch 3/10, Loss: 10222.5964\n",
            "Accuracy: 0.8023\n",
            "Epoch 4/10, Loss: 9705.0763\n",
            "Accuracy: 0.8050\n",
            "Epoch 5/10, Loss: 9305.7656\n",
            "Accuracy: 0.8083\n",
            "Epoch 6/10, Loss: 8991.1883\n",
            "Accuracy: 0.8112\n",
            "Epoch 7/10, Loss: 8724.7808\n",
            "Accuracy: 0.8141\n",
            "Epoch 8/10, Loss: 8492.1851\n",
            "Accuracy: 0.8166\n",
            "Epoch 9/10, Loss: 8286.8527\n",
            "Accuracy: 0.8162\n",
            "Epoch 10/10, Loss: 8106.6331\n",
            "Accuracy: 0.8181\n",
            "\n",
            "Running experiment with window_size=0, hidden_layers=2, layer_widths=[128, 256], activation=relu\n",
            "Epoch 1/10, Loss: 14708.4204\n",
            "Accuracy: 0.8112\n",
            "Epoch 2/10, Loss: 9415.5522\n",
            "Accuracy: 0.8278\n",
            "Epoch 3/10, Loss: 8570.6643\n",
            "Accuracy: 0.8374\n",
            "Epoch 4/10, Loss: 8046.3583\n",
            "Accuracy: 0.8392\n",
            "Epoch 5/10, Loss: 7673.2374\n",
            "Accuracy: 0.8413\n",
            "Epoch 6/10, Loss: 7365.7058\n",
            "Accuracy: 0.8430\n",
            "Epoch 7/10, Loss: 7132.2032\n",
            "Accuracy: 0.8409\n",
            "Epoch 8/10, Loss: 6919.0703\n",
            "Accuracy: 0.8415\n",
            "Epoch 9/10, Loss: 6743.8438\n",
            "Accuracy: 0.8399\n",
            "Epoch 10/10, Loss: 6592.0752\n",
            "Accuracy: 0.8411\n",
            "\n",
            "Running experiment with window_size=0, hidden_layers=2, layer_widths=[128, 256], activation=sigmoid\n",
            "Epoch 1/10, Loss: 45224.7552\n",
            "Accuracy: 0.3669\n",
            "Epoch 2/10, Loss: 26815.7304\n",
            "Accuracy: 0.6646\n",
            "Epoch 3/10, Loss: 17245.0678\n",
            "Accuracy: 0.7138\n",
            "Epoch 4/10, Loss: 14239.8540\n",
            "Accuracy: 0.7364\n",
            "Epoch 5/10, Loss: 13073.9333\n",
            "Accuracy: 0.7517\n",
            "Epoch 6/10, Loss: 12392.0044\n",
            "Accuracy: 0.7660\n",
            "Epoch 7/10, Loss: 11850.9704\n",
            "Accuracy: 0.7872\n",
            "Epoch 8/10, Loss: 11389.6475\n",
            "Accuracy: 0.7978\n",
            "Epoch 9/10, Loss: 10999.7712\n",
            "Accuracy: 0.8038\n",
            "Epoch 10/10, Loss: 10673.3082\n",
            "Accuracy: 0.8063\n",
            "\n",
            "Running experiment with window_size=0, hidden_layers=2, layer_widths=[128, 256], activation=identity\n",
            "Epoch 1/10, Loss: 14718.0822\n",
            "Accuracy: 0.7828\n",
            "Epoch 2/10, Loss: 11652.2461\n",
            "Accuracy: 0.7843\n",
            "Epoch 3/10, Loss: 11373.7540\n",
            "Accuracy: 0.7861\n",
            "Epoch 4/10, Loss: 11249.9211\n",
            "Accuracy: 0.7847\n",
            "Epoch 5/10, Loss: 11190.3019\n",
            "Accuracy: 0.7841\n",
            "Epoch 6/10, Loss: 11139.4838\n",
            "Accuracy: 0.7843\n",
            "Epoch 7/10, Loss: 11129.7797\n",
            "Accuracy: 0.7837\n",
            "Epoch 8/10, Loss: 11098.1950\n",
            "Accuracy: 0.7837\n",
            "Epoch 9/10, Loss: 11099.8937\n",
            "Accuracy: 0.7837\n",
            "Epoch 10/10, Loss: 11068.3276\n",
            "Accuracy: 0.7828\n",
            "\n",
            "Running experiment with window_size=0, hidden_layers=2, layer_widths=[256, 512], activation=tanh\n",
            "Epoch 1/10, Loss: 14105.6129\n",
            "Accuracy: 0.7855\n",
            "Epoch 2/10, Loss: 11189.6507\n",
            "Accuracy: 0.7909\n",
            "Epoch 3/10, Loss: 10562.7267\n",
            "Accuracy: 0.7965\n",
            "Epoch 4/10, Loss: 10067.6246\n",
            "Accuracy: 0.8034\n",
            "Epoch 5/10, Loss: 9643.0088\n",
            "Accuracy: 0.8071\n",
            "Epoch 6/10, Loss: 9285.6889\n",
            "Accuracy: 0.8106\n",
            "Epoch 7/10, Loss: 8993.5242\n",
            "Accuracy: 0.8131\n",
            "Epoch 8/10, Loss: 8755.2646\n",
            "Accuracy: 0.8177\n",
            "Epoch 9/10, Loss: 8545.8409\n",
            "Accuracy: 0.8175\n",
            "Epoch 10/10, Loss: 8373.4845\n",
            "Accuracy: 0.8179\n",
            "\n",
            "Running experiment with window_size=0, hidden_layers=2, layer_widths=[256, 512], activation=relu\n",
            "Epoch 1/10, Loss: 14073.6959\n",
            "Accuracy: 0.8075\n",
            "Epoch 2/10, Loss: 9200.4445\n",
            "Accuracy: 0.8241\n",
            "Epoch 3/10, Loss: 8340.4805\n",
            "Accuracy: 0.8345\n",
            "Epoch 4/10, Loss: 7806.3354\n",
            "Accuracy: 0.8334\n",
            "Epoch 5/10, Loss: 7422.5117\n",
            "Accuracy: 0.8345\n",
            "Epoch 6/10, Loss: 7120.9444\n",
            "Accuracy: 0.8332\n",
            "Epoch 7/10, Loss: 6871.1489\n",
            "Accuracy: 0.8353\n",
            "Epoch 8/10, Loss: 6643.4667\n",
            "Accuracy: 0.8339\n",
            "Epoch 9/10, Loss: 6443.7178\n",
            "Accuracy: 0.8349\n",
            "Epoch 10/10, Loss: 6283.5889\n",
            "Accuracy: 0.8332\n",
            "\n",
            "Running experiment with window_size=0, hidden_layers=2, layer_widths=[256, 512], activation=sigmoid\n",
            "Epoch 1/10, Loss: 45470.8868\n",
            "Accuracy: 0.3470\n",
            "Epoch 2/10, Loss: 27847.4358\n",
            "Accuracy: 0.6333\n",
            "Epoch 3/10, Loss: 18797.5065\n",
            "Accuracy: 0.7104\n",
            "Epoch 4/10, Loss: 14973.2392\n",
            "Accuracy: 0.7349\n",
            "Epoch 5/10, Loss: 13401.6337\n",
            "Accuracy: 0.7467\n",
            "Epoch 6/10, Loss: 12572.6968\n",
            "Accuracy: 0.7639\n",
            "Epoch 7/10, Loss: 11969.3072\n",
            "Accuracy: 0.7749\n",
            "Epoch 8/10, Loss: 11470.9553\n",
            "Accuracy: 0.7934\n",
            "Epoch 9/10, Loss: 11059.1441\n",
            "Accuracy: 0.8005\n",
            "Epoch 10/10, Loss: 10724.8817\n",
            "Accuracy: 0.8044\n",
            "\n",
            "Running experiment with window_size=0, hidden_layers=2, layer_widths=[256, 512], activation=identity\n",
            "Epoch 1/10, Loss: 14187.6137\n",
            "Accuracy: 0.7818\n",
            "Epoch 2/10, Loss: 11662.8133\n",
            "Accuracy: 0.7853\n",
            "Epoch 3/10, Loss: 11398.7683\n",
            "Accuracy: 0.7857\n",
            "Epoch 4/10, Loss: 11274.6044\n",
            "Accuracy: 0.7847\n",
            "Epoch 5/10, Loss: 11215.9254\n",
            "Accuracy: 0.7843\n",
            "Epoch 6/10, Loss: 11161.4008\n",
            "Accuracy: 0.7847\n",
            "Epoch 7/10, Loss: 11151.6494\n",
            "Accuracy: 0.7834\n",
            "Epoch 8/10, Loss: 11114.4849\n",
            "Accuracy: 0.7832\n",
            "Epoch 9/10, Loss: 11114.8357\n",
            "Accuracy: 0.7834\n",
            "Epoch 10/10, Loss: 11081.4846\n",
            "Accuracy: 0.7828\n",
            "\n",
            "Running experiment with window_size=1, hidden_layers=0, layer_widths=[128], activation=tanh\n",
            "Epoch 1/10, Loss: 17857.8405\n",
            "Accuracy: 0.8085\n",
            "Epoch 2/10, Loss: 10872.8789\n",
            "Accuracy: 0.8326\n",
            "Epoch 3/10, Loss: 9638.0909\n",
            "Accuracy: 0.8407\n",
            "Epoch 4/10, Loss: 9016.1730\n",
            "Accuracy: 0.8436\n",
            "Epoch 5/10, Loss: 8618.5681\n",
            "Accuracy: 0.8457\n",
            "Epoch 6/10, Loss: 8334.3881\n",
            "Accuracy: 0.8473\n",
            "Epoch 7/10, Loss: 8117.5272\n",
            "Accuracy: 0.8496\n",
            "Epoch 8/10, Loss: 7944.6575\n",
            "Accuracy: 0.8496\n",
            "Epoch 9/10, Loss: 7802.4872\n",
            "Accuracy: 0.8496\n",
            "Epoch 10/10, Loss: 7682.7970\n",
            "Accuracy: 0.8507\n",
            "\n",
            "Running experiment with window_size=1, hidden_layers=0, layer_widths=[128], activation=relu\n",
            "Epoch 1/10, Loss: 17855.4740\n",
            "Accuracy: 0.8081\n",
            "Epoch 2/10, Loss: 10879.9402\n",
            "Accuracy: 0.8328\n",
            "Epoch 3/10, Loss: 9641.9486\n",
            "Accuracy: 0.8401\n",
            "Epoch 4/10, Loss: 9017.9847\n",
            "Accuracy: 0.8446\n",
            "Epoch 5/10, Loss: 8619.2248\n",
            "Accuracy: 0.8455\n",
            "Epoch 6/10, Loss: 8334.3773\n",
            "Accuracy: 0.8484\n",
            "Epoch 7/10, Loss: 8117.1206\n",
            "Accuracy: 0.8500\n",
            "Epoch 8/10, Loss: 7944.0133\n",
            "Accuracy: 0.8500\n",
            "Epoch 9/10, Loss: 7801.7020\n",
            "Accuracy: 0.8500\n",
            "Epoch 10/10, Loss: 7681.9328\n",
            "Accuracy: 0.8511\n",
            "\n",
            "Running experiment with window_size=1, hidden_layers=0, layer_widths=[128], activation=sigmoid\n",
            "Epoch 1/10, Loss: 17870.1719\n",
            "Accuracy: 0.8083\n",
            "Epoch 2/10, Loss: 10880.4101\n",
            "Accuracy: 0.8332\n",
            "Epoch 3/10, Loss: 9643.4880\n",
            "Accuracy: 0.8407\n",
            "Epoch 4/10, Loss: 9020.0776\n",
            "Accuracy: 0.8434\n",
            "Epoch 5/10, Loss: 8621.4350\n",
            "Accuracy: 0.8448\n",
            "Epoch 6/10, Loss: 8336.4704\n",
            "Accuracy: 0.8477\n",
            "Epoch 7/10, Loss: 8119.0175\n",
            "Accuracy: 0.8492\n",
            "Epoch 8/10, Loss: 7945.7057\n",
            "Accuracy: 0.8490\n",
            "Epoch 9/10, Loss: 7803.2061\n",
            "Accuracy: 0.8498\n",
            "Epoch 10/10, Loss: 7683.2703\n",
            "Accuracy: 0.8511\n",
            "\n",
            "Running experiment with window_size=1, hidden_layers=0, layer_widths=[128], activation=identity\n",
            "Epoch 1/10, Loss: 17883.0425\n",
            "Accuracy: 0.8088\n",
            "Epoch 2/10, Loss: 10884.1520\n",
            "Accuracy: 0.8324\n",
            "Epoch 3/10, Loss: 9643.6940\n",
            "Accuracy: 0.8409\n",
            "Epoch 4/10, Loss: 9019.3398\n",
            "Accuracy: 0.8434\n",
            "Epoch 5/10, Loss: 8620.4785\n",
            "Accuracy: 0.8455\n",
            "Epoch 6/10, Loss: 8335.5554\n",
            "Accuracy: 0.8475\n",
            "Epoch 7/10, Loss: 8118.2456\n",
            "Accuracy: 0.8496\n",
            "Epoch 8/10, Loss: 7945.1073\n",
            "Accuracy: 0.8494\n",
            "Epoch 9/10, Loss: 7802.7800\n",
            "Accuracy: 0.8502\n",
            "Epoch 10/10, Loss: 7683.0016\n",
            "Accuracy: 0.8511\n",
            "\n",
            "Running experiment with window_size=1, hidden_layers=1, layer_widths=[128], activation=tanh\n",
            "Epoch 1/10, Loss: 14061.1612\n",
            "Accuracy: 0.8125\n",
            "Epoch 2/10, Loss: 9116.3252\n",
            "Accuracy: 0.8224\n",
            "Epoch 3/10, Loss: 8357.5334\n",
            "Accuracy: 0.8224\n",
            "Epoch 4/10, Loss: 7860.5119\n",
            "Accuracy: 0.8235\n",
            "Epoch 5/10, Loss: 7458.1087\n",
            "Accuracy: 0.8303\n",
            "Epoch 6/10, Loss: 7099.6513\n",
            "Accuracy: 0.8330\n",
            "Epoch 7/10, Loss: 6763.5037\n",
            "Accuracy: 0.8357\n",
            "Epoch 8/10, Loss: 6438.9511\n",
            "Accuracy: 0.8372\n",
            "Epoch 9/10, Loss: 6123.4620\n",
            "Accuracy: 0.8380\n",
            "Epoch 10/10, Loss: 5817.6344\n",
            "Accuracy: 0.8368\n",
            "\n",
            "Running experiment with window_size=1, hidden_layers=1, layer_widths=[128], activation=relu\n",
            "Epoch 1/10, Loss: 14710.1739\n",
            "Accuracy: 0.8075\n",
            "Epoch 2/10, Loss: 8644.3212\n",
            "Accuracy: 0.8276\n",
            "Epoch 3/10, Loss: 7480.9402\n",
            "Accuracy: 0.8380\n",
            "Epoch 4/10, Loss: 6668.0831\n",
            "Accuracy: 0.8426\n",
            "Epoch 5/10, Loss: 5986.4658\n",
            "Accuracy: 0.8453\n",
            "Epoch 6/10, Loss: 5412.6386\n",
            "Accuracy: 0.8467\n",
            "Epoch 7/10, Loss: 4927.2956\n",
            "Accuracy: 0.8463\n",
            "Epoch 8/10, Loss: 4477.5952\n",
            "Accuracy: 0.8513\n",
            "Epoch 9/10, Loss: 4056.6335\n",
            "Accuracy: 0.8498\n",
            "Epoch 10/10, Loss: 3701.1903\n",
            "Accuracy: 0.8504\n",
            "\n",
            "Running experiment with window_size=1, hidden_layers=1, layer_widths=[128], activation=sigmoid\n",
            "Epoch 1/10, Loss: 26517.5311\n",
            "Accuracy: 0.7438\n",
            "Epoch 2/10, Loss: 12410.1221\n",
            "Accuracy: 0.7774\n",
            "Epoch 3/10, Loss: 10265.6484\n",
            "Accuracy: 0.8021\n",
            "Epoch 4/10, Loss: 9268.8186\n",
            "Accuracy: 0.8187\n",
            "Epoch 5/10, Loss: 8691.5974\n",
            "Accuracy: 0.8272\n",
            "Epoch 6/10, Loss: 8302.6752\n",
            "Accuracy: 0.8328\n",
            "Epoch 7/10, Loss: 8007.7093\n",
            "Accuracy: 0.8368\n",
            "Epoch 8/10, Loss: 7765.3736\n",
            "Accuracy: 0.8374\n",
            "Epoch 9/10, Loss: 7555.0935\n",
            "Accuracy: 0.8403\n",
            "Epoch 10/10, Loss: 7365.4906\n",
            "Accuracy: 0.8434\n",
            "\n",
            "Running experiment with window_size=1, hidden_layers=1, layer_widths=[128], activation=identity\n",
            "Epoch 1/10, Loss: 14107.9069\n",
            "Accuracy: 0.8083\n",
            "Epoch 2/10, Loss: 9435.7258\n",
            "Accuracy: 0.8175\n",
            "Epoch 3/10, Loss: 8851.1843\n",
            "Accuracy: 0.8216\n",
            "Epoch 4/10, Loss: 8546.6422\n",
            "Accuracy: 0.8220\n",
            "Epoch 5/10, Loss: 8355.5599\n",
            "Accuracy: 0.8216\n",
            "Epoch 6/10, Loss: 8225.1545\n",
            "Accuracy: 0.8212\n",
            "Epoch 7/10, Loss: 8128.1661\n",
            "Accuracy: 0.8208\n",
            "Epoch 8/10, Loss: 8058.8803\n",
            "Accuracy: 0.8210\n",
            "Epoch 9/10, Loss: 7997.9809\n",
            "Accuracy: 0.8216\n",
            "Epoch 10/10, Loss: 7955.1368\n",
            "Accuracy: 0.8210\n",
            "\n",
            "Running experiment with window_size=1, hidden_layers=1, layer_widths=[256], activation=tanh\n",
            "Epoch 1/10, Loss: 13703.8385\n",
            "Accuracy: 0.8104\n",
            "Epoch 2/10, Loss: 9237.1044\n",
            "Accuracy: 0.8171\n",
            "Epoch 3/10, Loss: 8574.7806\n",
            "Accuracy: 0.8235\n",
            "Epoch 4/10, Loss: 8153.3281\n",
            "Accuracy: 0.8237\n",
            "Epoch 5/10, Loss: 7829.9323\n",
            "Accuracy: 0.8280\n",
            "Epoch 6/10, Loss: 7554.2865\n",
            "Accuracy: 0.8297\n",
            "Epoch 7/10, Loss: 7304.7356\n",
            "Accuracy: 0.8328\n",
            "Epoch 8/10, Loss: 7066.8619\n",
            "Accuracy: 0.8353\n",
            "Epoch 9/10, Loss: 6832.2029\n",
            "Accuracy: 0.8382\n",
            "Epoch 10/10, Loss: 6598.0830\n",
            "Accuracy: 0.8378\n",
            "\n",
            "Running experiment with window_size=1, hidden_layers=1, layer_widths=[256], activation=relu\n",
            "Epoch 1/10, Loss: 14253.3466\n",
            "Accuracy: 0.8133\n",
            "Epoch 2/10, Loss: 8449.6352\n",
            "Accuracy: 0.8330\n",
            "Epoch 3/10, Loss: 7222.7769\n",
            "Accuracy: 0.8421\n",
            "Epoch 4/10, Loss: 6340.5036\n",
            "Accuracy: 0.8490\n",
            "Epoch 5/10, Loss: 5618.5405\n",
            "Accuracy: 0.8540\n",
            "Epoch 6/10, Loss: 4965.5986\n",
            "Accuracy: 0.8538\n",
            "Epoch 7/10, Loss: 4425.7277\n",
            "Accuracy: 0.8598\n",
            "Epoch 8/10, Loss: 3927.6409\n",
            "Accuracy: 0.8590\n",
            "Epoch 9/10, Loss: 3489.0541\n",
            "Accuracy: 0.8594\n",
            "Epoch 10/10, Loss: 3113.8688\n",
            "Accuracy: 0.8604\n",
            "\n",
            "Running experiment with window_size=1, hidden_layers=1, layer_widths=[256], activation=sigmoid\n",
            "Epoch 1/10, Loss: 26099.0288\n",
            "Accuracy: 0.7430\n",
            "Epoch 2/10, Loss: 12394.5454\n",
            "Accuracy: 0.7714\n",
            "Epoch 3/10, Loss: 10343.5819\n",
            "Accuracy: 0.7934\n",
            "Epoch 4/10, Loss: 9382.3514\n",
            "Accuracy: 0.8083\n",
            "Epoch 5/10, Loss: 8821.9363\n",
            "Accuracy: 0.8195\n",
            "Epoch 6/10, Loss: 8438.1482\n",
            "Accuracy: 0.8264\n",
            "Epoch 7/10, Loss: 8143.5401\n",
            "Accuracy: 0.8301\n",
            "Epoch 8/10, Loss: 7900.0866\n",
            "Accuracy: 0.8328\n",
            "Epoch 9/10, Loss: 7688.8756\n",
            "Accuracy: 0.8361\n",
            "Epoch 10/10, Loss: 7499.4049\n",
            "Accuracy: 0.8378\n",
            "\n",
            "Running experiment with window_size=1, hidden_layers=1, layer_widths=[256], activation=identity\n",
            "Epoch 1/10, Loss: 13686.5220\n",
            "Accuracy: 0.8092\n",
            "Epoch 2/10, Loss: 9384.9836\n",
            "Accuracy: 0.8166\n",
            "Epoch 3/10, Loss: 8830.7551\n",
            "Accuracy: 0.8195\n",
            "Epoch 4/10, Loss: 8532.7562\n",
            "Accuracy: 0.8216\n",
            "Epoch 5/10, Loss: 8346.1998\n",
            "Accuracy: 0.8212\n",
            "Epoch 6/10, Loss: 8219.4606\n",
            "Accuracy: 0.8195\n",
            "Epoch 7/10, Loss: 8125.1600\n",
            "Accuracy: 0.8206\n",
            "Epoch 8/10, Loss: 8057.5284\n",
            "Accuracy: 0.8212\n",
            "Epoch 9/10, Loss: 7999.0612\n",
            "Accuracy: 0.8214\n",
            "Epoch 10/10, Loss: 7957.9802\n",
            "Accuracy: 0.8218\n",
            "\n",
            "Running experiment with window_size=1, hidden_layers=1, layer_widths=[512], activation=tanh\n",
            "Epoch 1/10, Loss: 13225.9459\n",
            "Accuracy: 0.8114\n",
            "Epoch 2/10, Loss: 9292.8288\n",
            "Accuracy: 0.8181\n",
            "Epoch 3/10, Loss: 8730.5761\n",
            "Accuracy: 0.8204\n",
            "Epoch 4/10, Loss: 8409.0850\n",
            "Accuracy: 0.8212\n",
            "Epoch 5/10, Loss: 8190.0794\n",
            "Accuracy: 0.8218\n",
            "Epoch 6/10, Loss: 8017.2937\n",
            "Accuracy: 0.8216\n",
            "Epoch 7/10, Loss: 7866.8275\n",
            "Accuracy: 0.8212\n",
            "Epoch 8/10, Loss: 7724.5388\n",
            "Accuracy: 0.8212\n",
            "Epoch 9/10, Loss: 7583.7507\n",
            "Accuracy: 0.8214\n",
            "Epoch 10/10, Loss: 7441.2934\n",
            "Accuracy: 0.8202\n",
            "\n",
            "Running experiment with window_size=1, hidden_layers=1, layer_widths=[512], activation=relu\n",
            "Epoch 1/10, Loss: 13835.0015\n",
            "Accuracy: 0.8125\n",
            "Epoch 2/10, Loss: 8309.9226\n",
            "Accuracy: 0.8320\n",
            "Epoch 3/10, Loss: 7014.9464\n",
            "Accuracy: 0.8421\n",
            "Epoch 4/10, Loss: 6070.8188\n",
            "Accuracy: 0.8486\n",
            "Epoch 5/10, Loss: 5280.0422\n",
            "Accuracy: 0.8525\n",
            "Epoch 6/10, Loss: 4567.8292\n",
            "Accuracy: 0.8525\n",
            "Epoch 7/10, Loss: 3984.5669\n",
            "Accuracy: 0.8554\n",
            "Epoch 8/10, Loss: 3453.7513\n",
            "Accuracy: 0.8567\n",
            "Epoch 9/10, Loss: 2998.3193\n",
            "Accuracy: 0.8556\n",
            "Epoch 10/10, Loss: 2623.2436\n",
            "Accuracy: 0.8587\n",
            "\n",
            "Running experiment with window_size=1, hidden_layers=1, layer_widths=[512], activation=sigmoid\n",
            "Epoch 1/10, Loss: 26795.8074\n",
            "Accuracy: 0.7133\n",
            "Epoch 2/10, Loss: 12870.5215\n",
            "Accuracy: 0.7455\n",
            "Epoch 3/10, Loss: 10751.6913\n",
            "Accuracy: 0.7671\n",
            "Epoch 4/10, Loss: 9761.8420\n",
            "Accuracy: 0.7949\n",
            "Epoch 5/10, Loss: 9176.4948\n",
            "Accuracy: 0.8063\n",
            "Epoch 6/10, Loss: 8769.6882\n",
            "Accuracy: 0.8164\n",
            "Epoch 7/10, Loss: 8456.8090\n",
            "Accuracy: 0.8229\n",
            "Epoch 8/10, Loss: 8199.7816\n",
            "Accuracy: 0.8270\n",
            "Epoch 9/10, Loss: 7978.9271\n",
            "Accuracy: 0.8301\n",
            "Epoch 10/10, Loss: 7783.0128\n",
            "Accuracy: 0.8312\n",
            "\n",
            "Running experiment with window_size=1, hidden_layers=1, layer_widths=[512], activation=identity\n",
            "Epoch 1/10, Loss: 13155.8521\n",
            "Accuracy: 0.8100\n",
            "Epoch 2/10, Loss: 9364.4362\n",
            "Accuracy: 0.8137\n",
            "Epoch 3/10, Loss: 8843.4481\n",
            "Accuracy: 0.8193\n",
            "Epoch 4/10, Loss: 8555.6171\n",
            "Accuracy: 0.8197\n",
            "Epoch 5/10, Loss: 8376.1269\n",
            "Accuracy: 0.8193\n",
            "Epoch 6/10, Loss: 8251.3780\n",
            "Accuracy: 0.8204\n",
            "Epoch 7/10, Loss: 8157.2557\n",
            "Accuracy: 0.8204\n",
            "Epoch 8/10, Loss: 8089.9396\n",
            "Accuracy: 0.8214\n",
            "Epoch 9/10, Loss: 8030.7892\n",
            "Accuracy: 0.8204\n",
            "Epoch 10/10, Loss: 7987.6047\n",
            "Accuracy: 0.8202\n",
            "\n",
            "Running experiment with window_size=1, hidden_layers=2, layer_widths=[128, 256], activation=tanh\n",
            "Epoch 1/10, Loss: 14445.4242\n",
            "Accuracy: 0.7675\n",
            "Epoch 2/10, Loss: 9807.7829\n",
            "Accuracy: 0.7859\n",
            "Epoch 3/10, Loss: 8910.1050\n",
            "Accuracy: 0.8038\n",
            "Epoch 4/10, Loss: 8241.7325\n",
            "Accuracy: 0.8100\n",
            "Epoch 5/10, Loss: 7672.6735\n",
            "Accuracy: 0.8154\n",
            "Epoch 6/10, Loss: 7146.5606\n",
            "Accuracy: 0.8173\n",
            "Epoch 7/10, Loss: 6654.4095\n",
            "Accuracy: 0.8183\n",
            "Epoch 8/10, Loss: 6241.0791\n",
            "Accuracy: 0.8100\n",
            "Epoch 9/10, Loss: 5900.8212\n",
            "Accuracy: 0.8227\n",
            "Epoch 10/10, Loss: 5496.0826\n",
            "Accuracy: 0.8247\n",
            "\n",
            "Running experiment with window_size=1, hidden_layers=2, layer_widths=[128, 256], activation=relu\n",
            "Epoch 1/10, Loss: 15528.6578\n",
            "Accuracy: 0.7778\n",
            "Epoch 2/10, Loss: 8703.3317\n",
            "Accuracy: 0.8152\n",
            "Epoch 3/10, Loss: 7251.0951\n",
            "Accuracy: 0.8287\n",
            "Epoch 4/10, Loss: 6233.3714\n",
            "Accuracy: 0.8382\n",
            "Epoch 5/10, Loss: 5482.3447\n",
            "Accuracy: 0.8417\n",
            "Epoch 6/10, Loss: 4853.5811\n",
            "Accuracy: 0.8438\n",
            "Epoch 7/10, Loss: 4308.4535\n",
            "Accuracy: 0.8448\n",
            "Epoch 8/10, Loss: 3837.4174\n",
            "Accuracy: 0.8392\n",
            "Epoch 9/10, Loss: 3566.9382\n",
            "Accuracy: 0.8436\n",
            "Epoch 10/10, Loss: 3229.5748\n",
            "Accuracy: 0.8397\n",
            "\n",
            "Running experiment with window_size=1, hidden_layers=2, layer_widths=[128, 256], activation=sigmoid\n",
            "Epoch 1/10, Loss: 45345.0556\n",
            "Accuracy: 0.2377\n",
            "Epoch 2/10, Loss: 29459.2776\n",
            "Accuracy: 0.6067\n",
            "Epoch 3/10, Loss: 17920.3553\n",
            "Accuracy: 0.7146\n",
            "Epoch 4/10, Loss: 13757.0168\n",
            "Accuracy: 0.7426\n",
            "Epoch 5/10, Loss: 12218.6566\n",
            "Accuracy: 0.7594\n",
            "Epoch 6/10, Loss: 11361.6780\n",
            "Accuracy: 0.7768\n",
            "Epoch 7/10, Loss: 10663.6859\n",
            "Accuracy: 0.7893\n",
            "Epoch 8/10, Loss: 10077.3716\n",
            "Accuracy: 0.8019\n",
            "Epoch 9/10, Loss: 9609.7787\n",
            "Accuracy: 0.8073\n",
            "Epoch 10/10, Loss: 9229.0535\n",
            "Accuracy: 0.8135\n",
            "\n",
            "Running experiment with window_size=1, hidden_layers=2, layer_widths=[128, 256], activation=identity\n",
            "Epoch 1/10, Loss: 14584.9486\n",
            "Accuracy: 0.7639\n",
            "Epoch 2/10, Loss: 10551.6001\n",
            "Accuracy: 0.7720\n",
            "Epoch 3/10, Loss: 10116.7097\n",
            "Accuracy: 0.7868\n",
            "Epoch 4/10, Loss: 9944.6880\n",
            "Accuracy: 0.7888\n",
            "Epoch 5/10, Loss: 9839.3168\n",
            "Accuracy: 0.7909\n",
            "Epoch 6/10, Loss: 9805.7813\n",
            "Accuracy: 0.7915\n",
            "Epoch 7/10, Loss: 9764.9362\n",
            "Accuracy: 0.7895\n",
            "Epoch 8/10, Loss: 9757.0805\n",
            "Accuracy: 0.7899\n",
            "Epoch 9/10, Loss: 9737.3703\n",
            "Accuracy: 0.7899\n",
            "Epoch 10/10, Loss: 9745.8836\n",
            "Accuracy: 0.7907\n",
            "\n",
            "Running experiment with window_size=1, hidden_layers=2, layer_widths=[256, 512], activation=tanh\n",
            "Epoch 1/10, Loss: 14032.5734\n",
            "Accuracy: 0.7666\n",
            "Epoch 2/10, Loss: 10122.7892\n",
            "Accuracy: 0.7803\n",
            "Epoch 3/10, Loss: 9402.1891\n",
            "Accuracy: 0.7990\n",
            "Epoch 4/10, Loss: 8853.0933\n",
            "Accuracy: 0.8056\n",
            "Epoch 5/10, Loss: 8347.8717\n",
            "Accuracy: 0.8112\n",
            "Epoch 6/10, Loss: 7836.4239\n",
            "Accuracy: 0.8127\n",
            "Epoch 7/10, Loss: 7336.4218\n",
            "Accuracy: 0.8173\n",
            "Epoch 8/10, Loss: 6968.3626\n",
            "Accuracy: 0.8108\n",
            "Epoch 9/10, Loss: 6580.5613\n",
            "Accuracy: 0.8168\n",
            "Epoch 10/10, Loss: 6264.7198\n",
            "Accuracy: 0.8187\n",
            "\n",
            "Running experiment with window_size=1, hidden_layers=2, layer_widths=[256, 512], activation=relu\n",
            "Epoch 1/10, Loss: 14898.2357\n",
            "Accuracy: 0.7878\n",
            "Epoch 2/10, Loss: 8374.3464\n",
            "Accuracy: 0.8146\n",
            "Epoch 3/10, Loss: 6865.9665\n",
            "Accuracy: 0.8351\n",
            "Epoch 4/10, Loss: 5763.4193\n",
            "Accuracy: 0.8326\n",
            "Epoch 5/10, Loss: 4881.3112\n",
            "Accuracy: 0.8469\n",
            "Epoch 6/10, Loss: 4112.8546\n",
            "Accuracy: 0.8384\n",
            "Epoch 7/10, Loss: 3559.8454\n",
            "Accuracy: 0.8411\n",
            "Epoch 8/10, Loss: 3026.6451\n",
            "Accuracy: 0.8527\n",
            "Epoch 9/10, Loss: 2878.0220\n",
            "Accuracy: 0.8446\n",
            "Epoch 10/10, Loss: 2694.7922\n",
            "Accuracy: 0.8438\n",
            "\n",
            "Running experiment with window_size=1, hidden_layers=2, layer_widths=[256, 512], activation=sigmoid\n",
            "Epoch 1/10, Loss: 45854.7842\n",
            "Accuracy: 0.2246\n",
            "Epoch 2/10, Loss: 33599.1540\n",
            "Accuracy: 0.5447\n",
            "Epoch 3/10, Loss: 20521.3039\n",
            "Accuracy: 0.6909\n",
            "Epoch 4/10, Loss: 14853.3589\n",
            "Accuracy: 0.7370\n",
            "Epoch 5/10, Loss: 12717.1456\n",
            "Accuracy: 0.7567\n",
            "Epoch 6/10, Loss: 11604.0450\n",
            "Accuracy: 0.7731\n",
            "Epoch 7/10, Loss: 10821.2008\n",
            "Accuracy: 0.7897\n",
            "Epoch 8/10, Loss: 10270.2049\n",
            "Accuracy: 0.7965\n",
            "Epoch 9/10, Loss: 9859.3391\n",
            "Accuracy: 0.8034\n",
            "Epoch 10/10, Loss: 9523.5573\n",
            "Accuracy: 0.8061\n",
            "\n",
            "Running experiment with window_size=1, hidden_layers=2, layer_widths=[256, 512], activation=identity\n",
            "Epoch 1/10, Loss: 14157.1848\n",
            "Accuracy: 0.7619\n",
            "Epoch 2/10, Loss: 10571.1264\n",
            "Accuracy: 0.7695\n",
            "Epoch 3/10, Loss: 10156.8804\n",
            "Accuracy: 0.7864\n",
            "Epoch 4/10, Loss: 9977.7239\n",
            "Accuracy: 0.7878\n",
            "Epoch 5/10, Loss: 9875.0558\n",
            "Accuracy: 0.7888\n",
            "Epoch 6/10, Loss: 9838.8850\n",
            "Accuracy: 0.7884\n",
            "Epoch 7/10, Loss: 9796.7155\n",
            "Accuracy: 0.7884\n",
            "Epoch 8/10, Loss: 9791.9675\n",
            "Accuracy: 0.7882\n",
            "Epoch 9/10, Loss: 9774.3923\n",
            "Accuracy: 0.7859\n",
            "Epoch 10/10, Loss: 9780.1931\n",
            "Accuracy: 0.7880\n",
            "\n",
            "Running experiment with window_size=2, hidden_layers=0, layer_widths=[128], activation=tanh\n",
            "Epoch 1/10, Loss: 17622.7117\n",
            "Accuracy: 0.8092\n",
            "Epoch 2/10, Loss: 10575.8520\n",
            "Accuracy: 0.8309\n",
            "Epoch 3/10, Loss: 9264.7936\n",
            "Accuracy: 0.8357\n",
            "Epoch 4/10, Loss: 8587.3021\n",
            "Accuracy: 0.8374\n",
            "Epoch 5/10, Loss: 8147.2015\n",
            "Accuracy: 0.8399\n",
            "Epoch 6/10, Loss: 7829.0335\n",
            "Accuracy: 0.8405\n",
            "Epoch 7/10, Loss: 7584.0456\n",
            "Accuracy: 0.8415\n",
            "Epoch 8/10, Loss: 7387.3196\n",
            "Accuracy: 0.8438\n",
            "Epoch 9/10, Loss: 7224.5238\n",
            "Accuracy: 0.8434\n",
            "Epoch 10/10, Loss: 7086.7163\n",
            "Accuracy: 0.8434\n",
            "\n",
            "Running experiment with window_size=2, hidden_layers=0, layer_widths=[128], activation=relu\n",
            "Epoch 1/10, Loss: 17611.4354\n",
            "Accuracy: 0.8108\n",
            "Epoch 2/10, Loss: 10562.5200\n",
            "Accuracy: 0.8305\n",
            "Epoch 3/10, Loss: 9255.5382\n",
            "Accuracy: 0.8357\n",
            "Epoch 4/10, Loss: 8580.6246\n",
            "Accuracy: 0.8392\n",
            "Epoch 5/10, Loss: 8142.1696\n",
            "Accuracy: 0.8388\n",
            "Epoch 6/10, Loss: 7825.0579\n",
            "Accuracy: 0.8415\n",
            "Epoch 7/10, Loss: 7580.7665\n",
            "Accuracy: 0.8428\n",
            "Epoch 8/10, Loss: 7384.5139\n",
            "Accuracy: 0.8432\n",
            "Epoch 9/10, Loss: 7222.0526\n",
            "Accuracy: 0.8440\n",
            "Epoch 10/10, Loss: 7084.4911\n",
            "Accuracy: 0.8444\n",
            "\n",
            "Running experiment with window_size=2, hidden_layers=0, layer_widths=[128], activation=sigmoid\n",
            "Epoch 1/10, Loss: 17624.7949\n",
            "Accuracy: 0.8102\n",
            "Epoch 2/10, Loss: 10572.6936\n",
            "Accuracy: 0.8297\n",
            "Epoch 3/10, Loss: 9263.3678\n",
            "Accuracy: 0.8339\n",
            "Epoch 4/10, Loss: 8586.2811\n",
            "Accuracy: 0.8382\n",
            "Epoch 5/10, Loss: 8146.3273\n",
            "Accuracy: 0.8399\n",
            "Epoch 6/10, Loss: 7828.2042\n",
            "Accuracy: 0.8407\n",
            "Epoch 7/10, Loss: 7583.2058\n",
            "Accuracy: 0.8417\n",
            "Epoch 8/10, Loss: 7386.4353\n",
            "Accuracy: 0.8432\n",
            "Epoch 9/10, Loss: 7223.5799\n",
            "Accuracy: 0.8436\n",
            "Epoch 10/10, Loss: 7085.7116\n",
            "Accuracy: 0.8436\n",
            "\n",
            "Running experiment with window_size=2, hidden_layers=0, layer_widths=[128], activation=identity\n",
            "Epoch 1/10, Loss: 17660.6093\n",
            "Accuracy: 0.8102\n",
            "Epoch 2/10, Loss: 10578.9646\n",
            "Accuracy: 0.8297\n",
            "Epoch 3/10, Loss: 9265.2947\n",
            "Accuracy: 0.8345\n",
            "Epoch 4/10, Loss: 8587.1378\n",
            "Accuracy: 0.8392\n",
            "Epoch 5/10, Loss: 8146.8087\n",
            "Accuracy: 0.8413\n",
            "Epoch 6/10, Loss: 7828.5402\n",
            "Accuracy: 0.8415\n",
            "Epoch 7/10, Loss: 7583.5033\n",
            "Accuracy: 0.8426\n",
            "Epoch 8/10, Loss: 7386.7477\n",
            "Accuracy: 0.8434\n",
            "Epoch 9/10, Loss: 7223.9333\n",
            "Accuracy: 0.8438\n",
            "Epoch 10/10, Loss: 7086.1127\n",
            "Accuracy: 0.8446\n",
            "\n",
            "Running experiment with window_size=2, hidden_layers=1, layer_widths=[128], activation=tanh\n",
            "Epoch 1/10, Loss: 14523.4103\n",
            "Accuracy: 0.8139\n",
            "Epoch 2/10, Loss: 9063.6011\n",
            "Accuracy: 0.8253\n",
            "Epoch 3/10, Loss: 8073.8406\n",
            "Accuracy: 0.8274\n",
            "Epoch 4/10, Loss: 7407.0993\n",
            "Accuracy: 0.8268\n",
            "Epoch 5/10, Loss: 6846.4414\n",
            "Accuracy: 0.8314\n",
            "Epoch 6/10, Loss: 6334.0819\n",
            "Accuracy: 0.8287\n",
            "Epoch 7/10, Loss: 5842.5290\n",
            "Accuracy: 0.8307\n",
            "Epoch 8/10, Loss: 5365.9442\n",
            "Accuracy: 0.8314\n",
            "Epoch 9/10, Loss: 4903.3395\n",
            "Accuracy: 0.8295\n",
            "Epoch 10/10, Loss: 4451.5614\n",
            "Accuracy: 0.8305\n",
            "\n",
            "Running experiment with window_size=2, hidden_layers=1, layer_widths=[128], activation=relu\n",
            "Epoch 1/10, Loss: 15127.5918\n",
            "Accuracy: 0.8065\n",
            "Epoch 2/10, Loss: 8616.2857\n",
            "Accuracy: 0.8243\n",
            "Epoch 3/10, Loss: 7159.1408\n",
            "Accuracy: 0.8351\n",
            "Epoch 4/10, Loss: 6165.1243\n",
            "Accuracy: 0.8365\n",
            "Epoch 5/10, Loss: 5305.3954\n",
            "Accuracy: 0.8368\n",
            "Epoch 6/10, Loss: 4595.0321\n",
            "Accuracy: 0.8363\n",
            "Epoch 7/10, Loss: 3888.8012\n",
            "Accuracy: 0.8392\n",
            "Epoch 8/10, Loss: 3294.4430\n",
            "Accuracy: 0.8390\n",
            "Epoch 9/10, Loss: 2765.9814\n",
            "Accuracy: 0.8363\n",
            "Epoch 10/10, Loss: 2312.6094\n",
            "Accuracy: 0.8403\n",
            "\n",
            "Running experiment with window_size=2, hidden_layers=1, layer_widths=[128], activation=sigmoid\n",
            "Epoch 1/10, Loss: 26442.3520\n",
            "Accuracy: 0.7548\n",
            "Epoch 2/10, Loss: 12295.2856\n",
            "Accuracy: 0.7866\n",
            "Epoch 3/10, Loss: 10037.2183\n",
            "Accuracy: 0.8119\n",
            "Epoch 4/10, Loss: 8945.5458\n",
            "Accuracy: 0.8285\n",
            "Epoch 5/10, Loss: 8295.9821\n",
            "Accuracy: 0.8363\n",
            "Epoch 6/10, Loss: 7845.1627\n",
            "Accuracy: 0.8399\n",
            "Epoch 7/10, Loss: 7493.5419\n",
            "Accuracy: 0.8415\n",
            "Epoch 8/10, Loss: 7197.9198\n",
            "Accuracy: 0.8428\n",
            "Epoch 9/10, Loss: 6936.9002\n",
            "Accuracy: 0.8442\n",
            "Epoch 10/10, Loss: 6698.4906\n",
            "Accuracy: 0.8459\n",
            "\n",
            "Running experiment with window_size=2, hidden_layers=1, layer_widths=[128], activation=identity\n",
            "Epoch 1/10, Loss: 14490.3293\n",
            "Accuracy: 0.8106\n",
            "Epoch 2/10, Loss: 9440.1019\n",
            "Accuracy: 0.8156\n",
            "Epoch 3/10, Loss: 8697.9764\n",
            "Accuracy: 0.8195\n",
            "Epoch 4/10, Loss: 8291.7409\n",
            "Accuracy: 0.8173\n",
            "Epoch 5/10, Loss: 8031.8953\n",
            "Accuracy: 0.8177\n",
            "Epoch 6/10, Loss: 7847.0016\n",
            "Accuracy: 0.8162\n",
            "Epoch 7/10, Loss: 7707.9982\n",
            "Accuracy: 0.8154\n",
            "Epoch 8/10, Loss: 7601.7594\n",
            "Accuracy: 0.8160\n",
            "Epoch 9/10, Loss: 7513.0294\n",
            "Accuracy: 0.8144\n",
            "Epoch 10/10, Loss: 7448.0557\n",
            "Accuracy: 0.8129\n",
            "\n",
            "Running experiment with window_size=2, hidden_layers=1, layer_widths=[256], activation=tanh\n",
            "Epoch 1/10, Loss: 14226.0300\n",
            "Accuracy: 0.8137\n",
            "Epoch 2/10, Loss: 9213.2687\n",
            "Accuracy: 0.8189\n",
            "Epoch 3/10, Loss: 8350.4508\n",
            "Accuracy: 0.8181\n",
            "Epoch 4/10, Loss: 7798.9268\n",
            "Accuracy: 0.8179\n",
            "Epoch 5/10, Loss: 7353.3800\n",
            "Accuracy: 0.8216\n",
            "Epoch 6/10, Loss: 6952.7712\n",
            "Accuracy: 0.8224\n",
            "Epoch 7/10, Loss: 6571.7411\n",
            "Accuracy: 0.8200\n",
            "Epoch 8/10, Loss: 6194.0158\n",
            "Accuracy: 0.8200\n",
            "Epoch 9/10, Loss: 5817.1188\n",
            "Accuracy: 0.8195\n",
            "Epoch 10/10, Loss: 5434.1170\n",
            "Accuracy: 0.8210\n",
            "\n",
            "Running experiment with window_size=2, hidden_layers=1, layer_widths=[256], activation=relu\n",
            "Epoch 1/10, Loss: 14829.0884\n",
            "Accuracy: 0.8104\n",
            "Epoch 2/10, Loss: 8427.6677\n",
            "Accuracy: 0.8328\n",
            "Epoch 3/10, Loss: 6850.3186\n",
            "Accuracy: 0.8440\n",
            "Epoch 4/10, Loss: 5708.8259\n",
            "Accuracy: 0.8430\n",
            "Epoch 5/10, Loss: 4746.2570\n",
            "Accuracy: 0.8448\n",
            "Epoch 6/10, Loss: 3852.5889\n",
            "Accuracy: 0.8432\n",
            "Epoch 7/10, Loss: 3118.9606\n",
            "Accuracy: 0.8467\n",
            "Epoch 8/10, Loss: 2521.6285\n",
            "Accuracy: 0.8467\n",
            "Epoch 9/10, Loss: 1971.8248\n",
            "Accuracy: 0.8502\n",
            "Epoch 10/10, Loss: 1635.7225\n",
            "Accuracy: 0.8405\n",
            "\n",
            "Running experiment with window_size=2, hidden_layers=1, layer_widths=[256], activation=sigmoid\n",
            "Epoch 1/10, Loss: 26606.6951\n",
            "Accuracy: 0.7519\n",
            "Epoch 2/10, Loss: 12411.4481\n",
            "Accuracy: 0.7808\n",
            "Epoch 3/10, Loss: 10195.0816\n",
            "Accuracy: 0.8023\n",
            "Epoch 4/10, Loss: 9118.9242\n",
            "Accuracy: 0.8200\n",
            "Epoch 5/10, Loss: 8469.3281\n",
            "Accuracy: 0.8291\n",
            "Epoch 6/10, Loss: 8013.2737\n",
            "Accuracy: 0.8345\n",
            "Epoch 7/10, Loss: 7655.5024\n",
            "Accuracy: 0.8365\n",
            "Epoch 8/10, Loss: 7354.8102\n",
            "Accuracy: 0.8388\n",
            "Epoch 9/10, Loss: 7090.6257\n",
            "Accuracy: 0.8399\n",
            "Epoch 10/10, Loss: 6851.1041\n",
            "Accuracy: 0.8409\n",
            "\n",
            "Running experiment with window_size=2, hidden_layers=1, layer_widths=[256], activation=identity\n",
            "Epoch 1/10, Loss: 14210.2938\n",
            "Accuracy: 0.8092\n",
            "Epoch 2/10, Loss: 9404.1922\n",
            "Accuracy: 0.8137\n",
            "Epoch 3/10, Loss: 8678.3133\n",
            "Accuracy: 0.8156\n",
            "Epoch 4/10, Loss: 8277.1105\n",
            "Accuracy: 0.8150\n",
            "Epoch 5/10, Loss: 8017.5859\n",
            "Accuracy: 0.8160\n",
            "Epoch 6/10, Loss: 7837.6156\n",
            "Accuracy: 0.8148\n",
            "Epoch 7/10, Loss: 7698.2440\n",
            "Accuracy: 0.8144\n",
            "Epoch 8/10, Loss: 7600.2831\n",
            "Accuracy: 0.8164\n",
            "Epoch 9/10, Loss: 7504.6157\n",
            "Accuracy: 0.8139\n",
            "Epoch 10/10, Loss: 7453.7500\n",
            "Accuracy: 0.8119\n",
            "\n",
            "Running experiment with window_size=2, hidden_layers=1, layer_widths=[512], activation=tanh\n",
            "Epoch 1/10, Loss: 13848.9687\n",
            "Accuracy: 0.8123\n",
            "Epoch 2/10, Loss: 9302.3052\n",
            "Accuracy: 0.8158\n",
            "Epoch 3/10, Loss: 8542.1414\n",
            "Accuracy: 0.8175\n",
            "Epoch 4/10, Loss: 8085.6423\n",
            "Accuracy: 0.8171\n",
            "Epoch 5/10, Loss: 7752.9468\n",
            "Accuracy: 0.8164\n",
            "Epoch 6/10, Loss: 7479.7942\n",
            "Accuracy: 0.8175\n",
            "Epoch 7/10, Loss: 7239.0911\n",
            "Accuracy: 0.8168\n",
            "Epoch 8/10, Loss: 7013.5888\n",
            "Accuracy: 0.8168\n",
            "Epoch 9/10, Loss: 6788.6555\n",
            "Accuracy: 0.8177\n",
            "Epoch 10/10, Loss: 6551.4881\n",
            "Accuracy: 0.8183\n",
            "\n",
            "Running experiment with window_size=2, hidden_layers=1, layer_widths=[512], activation=relu\n",
            "Epoch 1/10, Loss: 14509.2911\n",
            "Accuracy: 0.8144\n",
            "Epoch 2/10, Loss: 8264.3165\n",
            "Accuracy: 0.8336\n",
            "Epoch 3/10, Loss: 6621.1967\n",
            "Accuracy: 0.8444\n",
            "Epoch 4/10, Loss: 5379.2777\n",
            "Accuracy: 0.8448\n",
            "Epoch 5/10, Loss: 4314.5784\n",
            "Accuracy: 0.8463\n",
            "Epoch 6/10, Loss: 3400.0859\n",
            "Accuracy: 0.8451\n",
            "Epoch 7/10, Loss: 2594.5252\n",
            "Accuracy: 0.8463\n",
            "Epoch 8/10, Loss: 1981.4804\n",
            "Accuracy: 0.8486\n",
            "Epoch 9/10, Loss: 1603.4430\n",
            "Accuracy: 0.8494\n",
            "Epoch 10/10, Loss: 1232.7211\n",
            "Accuracy: 0.8486\n",
            "\n",
            "Running experiment with window_size=2, hidden_layers=1, layer_widths=[512], activation=sigmoid\n",
            "Epoch 1/10, Loss: 27614.5239\n",
            "Accuracy: 0.7374\n",
            "Epoch 2/10, Loss: 12848.3195\n",
            "Accuracy: 0.7662\n",
            "Epoch 3/10, Loss: 10540.7330\n",
            "Accuracy: 0.7890\n",
            "Epoch 4/10, Loss: 9413.8629\n",
            "Accuracy: 0.8069\n",
            "Epoch 5/10, Loss: 8736.7081\n",
            "Accuracy: 0.8166\n",
            "Epoch 6/10, Loss: 8261.0065\n",
            "Accuracy: 0.8243\n",
            "Epoch 7/10, Loss: 7890.4621\n",
            "Accuracy: 0.8299\n",
            "Epoch 8/10, Loss: 7582.4769\n",
            "Accuracy: 0.8320\n",
            "Epoch 9/10, Loss: 7315.1235\n",
            "Accuracy: 0.8328\n",
            "Epoch 10/10, Loss: 7075.8915\n",
            "Accuracy: 0.8349\n",
            "\n",
            "Running experiment with window_size=2, hidden_layers=1, layer_widths=[512], activation=identity\n",
            "Epoch 1/10, Loss: 13876.4998\n",
            "Accuracy: 0.8088\n",
            "Epoch 2/10, Loss: 9400.7100\n",
            "Accuracy: 0.8114\n",
            "Epoch 3/10, Loss: 8698.0301\n",
            "Accuracy: 0.8104\n",
            "Epoch 4/10, Loss: 8303.3820\n",
            "Accuracy: 0.8112\n",
            "Epoch 5/10, Loss: 8046.9653\n",
            "Accuracy: 0.8139\n",
            "Epoch 6/10, Loss: 7868.4167\n",
            "Accuracy: 0.8137\n",
            "Epoch 7/10, Loss: 7727.3670\n",
            "Accuracy: 0.8146\n",
            "Epoch 8/10, Loss: 7629.8587\n",
            "Accuracy: 0.8152\n",
            "Epoch 9/10, Loss: 7533.1447\n",
            "Accuracy: 0.8135\n",
            "Epoch 10/10, Loss: 7483.4002\n",
            "Accuracy: 0.8119\n",
            "\n",
            "Running experiment with window_size=2, hidden_layers=2, layer_widths=[128, 256], activation=tanh\n",
            "Epoch 1/10, Loss: 15126.1002\n",
            "Accuracy: 0.7704\n",
            "Epoch 2/10, Loss: 9854.7298\n",
            "Accuracy: 0.7938\n",
            "Epoch 3/10, Loss: 8780.1966\n",
            "Accuracy: 0.8040\n",
            "Epoch 4/10, Loss: 7998.5980\n",
            "Accuracy: 0.8061\n",
            "Epoch 5/10, Loss: 7268.9637\n",
            "Accuracy: 0.8056\n",
            "Epoch 6/10, Loss: 6604.2038\n",
            "Accuracy: 0.8077\n",
            "Epoch 7/10, Loss: 5951.6983\n",
            "Accuracy: 0.8090\n",
            "Epoch 8/10, Loss: 5419.7685\n",
            "Accuracy: 0.8044\n",
            "Epoch 9/10, Loss: 4999.3830\n",
            "Accuracy: 0.8160\n",
            "Epoch 10/10, Loss: 4613.4847\n",
            "Accuracy: 0.8096\n",
            "\n",
            "Running experiment with window_size=2, hidden_layers=2, layer_widths=[128, 256], activation=relu\n",
            "Epoch 1/10, Loss: 16404.3839\n",
            "Accuracy: 0.7797\n",
            "Epoch 2/10, Loss: 9041.9586\n",
            "Accuracy: 0.8158\n",
            "Epoch 3/10, Loss: 7255.2857\n",
            "Accuracy: 0.8220\n",
            "Epoch 4/10, Loss: 6101.3034\n",
            "Accuracy: 0.8363\n",
            "Epoch 5/10, Loss: 5231.0643\n",
            "Accuracy: 0.8341\n",
            "Epoch 6/10, Loss: 4475.1448\n",
            "Accuracy: 0.8365\n",
            "Epoch 7/10, Loss: 3705.5392\n",
            "Accuracy: 0.8417\n",
            "Epoch 8/10, Loss: 3635.0404\n",
            "Accuracy: 0.8303\n",
            "Epoch 9/10, Loss: 3297.2430\n",
            "Accuracy: 0.8361\n",
            "Epoch 10/10, Loss: 3138.6820\n",
            "Accuracy: 0.8268\n",
            "\n",
            "Running experiment with window_size=2, hidden_layers=2, layer_widths=[128, 256], activation=sigmoid\n",
            "Epoch 1/10, Loss: 45530.8811\n",
            "Accuracy: 0.2286\n",
            "Epoch 2/10, Loss: 31157.3283\n",
            "Accuracy: 0.5907\n",
            "Epoch 3/10, Loss: 19248.7363\n",
            "Accuracy: 0.7096\n",
            "Epoch 4/10, Loss: 14417.4578\n",
            "Accuracy: 0.7527\n",
            "Epoch 5/10, Loss: 12467.3093\n",
            "Accuracy: 0.7689\n",
            "Epoch 6/10, Loss: 11450.4468\n",
            "Accuracy: 0.7799\n",
            "Epoch 7/10, Loss: 10659.2563\n",
            "Accuracy: 0.7930\n",
            "Epoch 8/10, Loss: 9999.1880\n",
            "Accuracy: 0.8017\n",
            "Epoch 9/10, Loss: 9466.1900\n",
            "Accuracy: 0.8098\n",
            "Epoch 10/10, Loss: 9022.2374\n",
            "Accuracy: 0.8152\n",
            "\n",
            "Running experiment with window_size=2, hidden_layers=2, layer_widths=[128, 256], activation=identity\n",
            "Epoch 1/10, Loss: 15438.9207\n",
            "Accuracy: 0.7613\n",
            "Epoch 2/10, Loss: 10768.2714\n",
            "Accuracy: 0.7700\n",
            "Epoch 3/10, Loss: 10215.0795\n",
            "Accuracy: 0.7679\n",
            "Epoch 4/10, Loss: 10020.5387\n",
            "Accuracy: 0.7633\n",
            "Epoch 5/10, Loss: 9983.0680\n",
            "Accuracy: 0.7604\n",
            "Epoch 6/10, Loss: 9937.2896\n",
            "Accuracy: 0.7695\n",
            "Epoch 7/10, Loss: 9947.5737\n",
            "Accuracy: 0.7563\n",
            "Epoch 8/10, Loss: 9978.4020\n",
            "Accuracy: 0.7530\n",
            "Epoch 9/10, Loss: 10066.8808\n",
            "Accuracy: 0.7511\n",
            "Epoch 10/10, Loss: 10114.3206\n",
            "Accuracy: 0.7527\n",
            "\n",
            "Running experiment with window_size=2, hidden_layers=2, layer_widths=[256, 512], activation=tanh\n",
            "Epoch 1/10, Loss: 14864.3786\n",
            "Accuracy: 0.7664\n",
            "Epoch 2/10, Loss: 10244.7729\n",
            "Accuracy: 0.7752\n",
            "Epoch 3/10, Loss: 9349.9930\n",
            "Accuracy: 0.7772\n",
            "Epoch 4/10, Loss: 8699.8269\n",
            "Accuracy: 0.7756\n",
            "Epoch 5/10, Loss: 8067.0438\n",
            "Accuracy: 0.7861\n",
            "Epoch 6/10, Loss: 7499.0170\n",
            "Accuracy: 0.7963\n",
            "Epoch 7/10, Loss: 7010.9434\n",
            "Accuracy: 0.7944\n",
            "Epoch 8/10, Loss: 6498.3746\n",
            "Accuracy: 0.8007\n",
            "Epoch 9/10, Loss: 6075.2399\n",
            "Accuracy: 0.7940\n",
            "Epoch 10/10, Loss: 5818.8702\n",
            "Accuracy: 0.7998\n",
            "\n",
            "Running experiment with window_size=2, hidden_layers=2, layer_widths=[256, 512], activation=relu\n",
            "Epoch 1/10, Loss: 15770.2576\n",
            "Accuracy: 0.7847\n",
            "Epoch 2/10, Loss: 8654.4397\n",
            "Accuracy: 0.8135\n",
            "Epoch 3/10, Loss: 6795.7003\n",
            "Accuracy: 0.8266\n",
            "Epoch 4/10, Loss: 5402.0143\n",
            "Accuracy: 0.8372\n",
            "Epoch 5/10, Loss: 4333.4106\n",
            "Accuracy: 0.8324\n",
            "Epoch 6/10, Loss: 3607.5626\n",
            "Accuracy: 0.8324\n",
            "Epoch 7/10, Loss: 3142.6915\n",
            "Accuracy: 0.8365\n",
            "Epoch 8/10, Loss: 3001.1622\n",
            "Accuracy: 0.8324\n",
            "Epoch 9/10, Loss: 2331.0846\n",
            "Accuracy: 0.8376\n",
            "Epoch 10/10, Loss: 2134.9771\n",
            "Accuracy: 0.8390\n",
            "\n",
            "Running experiment with window_size=2, hidden_layers=2, layer_widths=[256, 512], activation=sigmoid\n",
            "Epoch 1/10, Loss: 45810.3208\n",
            "Accuracy: 0.2209\n",
            "Epoch 2/10, Loss: 36052.6947\n",
            "Accuracy: 0.5115\n",
            "Epoch 3/10, Loss: 22552.0571\n",
            "Accuracy: 0.6729\n",
            "Epoch 4/10, Loss: 15910.3687\n",
            "Accuracy: 0.7397\n",
            "Epoch 5/10, Loss: 13135.0104\n",
            "Accuracy: 0.7586\n",
            "Epoch 6/10, Loss: 11905.7308\n",
            "Accuracy: 0.7733\n",
            "Epoch 7/10, Loss: 11057.3383\n",
            "Accuracy: 0.7866\n",
            "Epoch 8/10, Loss: 10413.1107\n",
            "Accuracy: 0.7944\n",
            "Epoch 9/10, Loss: 9903.7360\n",
            "Accuracy: 0.7982\n",
            "Epoch 10/10, Loss: 9483.2444\n",
            "Accuracy: 0.8023\n",
            "\n",
            "Running experiment with window_size=2, hidden_layers=2, layer_widths=[256, 512], activation=identity\n",
            "Epoch 1/10, Loss: 15084.1372\n",
            "Accuracy: 0.7583\n",
            "Epoch 2/10, Loss: 10822.8753\n",
            "Accuracy: 0.7660\n",
            "Epoch 3/10, Loss: 10271.7863\n",
            "Accuracy: 0.7662\n",
            "Epoch 4/10, Loss: 10094.6579\n",
            "Accuracy: 0.7619\n",
            "Epoch 5/10, Loss: 10042.5179\n",
            "Accuracy: 0.7615\n",
            "Epoch 6/10, Loss: 10039.0284\n",
            "Accuracy: 0.7571\n",
            "Epoch 7/10, Loss: 10056.2066\n",
            "Accuracy: 0.7542\n",
            "Epoch 8/10, Loss: 10144.6782\n",
            "Accuracy: 0.7498\n",
            "Epoch 9/10, Loss: 10157.6015\n",
            "Accuracy: 0.7532\n",
            "Epoch 10/10, Loss: 10267.6899\n",
            "Accuracy: 0.7501\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}